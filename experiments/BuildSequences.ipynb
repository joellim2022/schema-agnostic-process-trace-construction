{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685b052e-88dc-490d-96b5-33ea95b7aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === USER CONFIG ===\n",
    "\n",
    "# # 1) Profiling output from Section 3.1\n",
    "# profiling_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\profileprimitives_fooddeliver.csv\"   # ðŸ‘ˆ change as needed\n",
    "\n",
    "# # 2) Relationship scores from Section 3.2\n",
    "# relationship_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\relationship_scores.csv\"          # ðŸ‘ˆ change as needed\n",
    "\n",
    "# # 3) Folder containing raw CSV tables\n",
    "# data_folder = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\food_deliver\"                                      # ðŸ‘ˆ change as needed\n",
    "\n",
    "# # 4) Output CSV for traces\n",
    "# traces_output_csv = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\case_traces.csv\"                      # ðŸ‘ˆ change as needed\n",
    "\n",
    "\n",
    "# 1) Profiling output from Section 3.1\n",
    "profiling_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\ProfilePrimitives_TPCH.csv\"   # ðŸ‘ˆ change as needed\n",
    "\n",
    "# 2) Relationship scores from Section 3.2\n",
    "relationship_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\relationship_scores.csv\"          # ðŸ‘ˆ change as needed\n",
    "\n",
    "# 3) Folder containing raw CSV tables\n",
    "data_folder = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\tpch_csv\"                                      # ðŸ‘ˆ change as needed\n",
    "\n",
    "# 4) Output CSV for traces\n",
    "traces_output_csv = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\case_traces.csv\"                      # ðŸ‘ˆ change as needed\n",
    "\n",
    "\n",
    "# 5) How many cases to sample\n",
    "N_CASES_TO_SAMPLE = 100\n",
    "\n",
    "# Optional: random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813896ca-1b59-421f-a08d-357087a3af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK candidates per table:\n",
      "  customer: ['c_custkey', 'c_name', 'c_phone']\n",
      "  nation: ['n_nationkey', 'n_comment', 'n_name']\n",
      "  orders: ['o_orderkey', 'o_totalprice', 'o_comment']\n",
      "  part: ['p_partkey', 'p_name']\n",
      "  partsupp: ['ps_comment']\n",
      "  region: ['r_regionkey', 'r_name', 'r_comment']\n",
      "  supplier: ['s_suppkey', 's_name', 's_phone']\n",
      "\n",
      "Datetime candidates per table:\n",
      "  customer: ['c_acctbal', 'c_custkey', 'c_nationkey']\n",
      "  lineitem: ['l_receiptdate', 'l_orderkey', 'l_shipdate']\n",
      "  nation: ['n_nationkey', 'n_regionkey']\n",
      "  orders: ['o_orderkey', 'o_orderdate', 'o_totalprice']\n",
      "  part: ['p_partkey', 'p_retailprice', 'p_size']\n",
      "  partsupp: ['ps_partkey', 'ps_suppkey', 'ps_availqty']\n",
      "  region: ['r_regionkey']\n",
      "  supplier: ['s_acctbal', 's_suppkey', 's_nationkey']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles = pd.read_csv(profiling_csv_path)\n",
    "relationships = pd.read_csv(relationship_csv_path)\n",
    "\n",
    "# Make sure boolean flags are really booleans\n",
    "for col in [\"is_pk_candidate\", \"is_dt_candidate\"]:\n",
    "    if col in profiles.columns:\n",
    "        profiles[col] = profiles[col].astype(bool)\n",
    "    else:\n",
    "        profiles[col] = False  # fallback if missing\n",
    "# PK candidates per table, sorted by s_pk descending\n",
    "if \"s_pk\" not in profiles.columns:\n",
    "    raise ValueError(\"Profiling CSV is missing 's_pk' column.\")\n",
    "\n",
    "pk_cols_by_table: Dict[str, List[str]] = (\n",
    "    profiles[profiles[\"is_pk_candidate\"]]\n",
    "    .sort_values([\"table\", \"s_pk\"], ascending=[True, False])\n",
    "    .groupby(\"table\")[\"column\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Datetime candidates per table, sorted by s_dt descending\n",
    "if \"is_dt_candidate\" not in profiles.columns or \"s_dt\" not in profiles.columns:\n",
    "    raise ValueError(\"Profiling CSV must have 'is_dt_candidate' and 's_dt'.\")\n",
    "\n",
    "dt_cols_by_table: Dict[str, List[str]] = (\n",
    "    profiles[profiles[\"is_dt_candidate\"]]\n",
    "    .sort_values([\"table\", \"s_dt\"], ascending=[True, False])\n",
    "    .groupby(\"table\")[\"column\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"PK candidates per table:\")\n",
    "for t, cols in pk_cols_by_table.items():\n",
    "    print(f\"  {t}: {cols}\")\n",
    "\n",
    "print(\"\\nDatetime candidates per table:\")\n",
    "for t, cols in dt_cols_by_table.items():\n",
    "    print(f\"  {t}: {cols}\")\n",
    "\n",
    "\n",
    "required_rel_cols = {\"table_i\", \"column_i\", \"table_k\", \"column_k\", \"s\"}\n",
    "missing = required_rel_cols - set(relationships.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Relationship CSV is missing columns: {missing}\")\n",
    "\n",
    "# Build adjacency list: (table, column) -> list of (neighbor_table, neighbor_column, score)\n",
    "from collections import defaultdict\n",
    "\n",
    "adjacency: Dict[Tuple[str, str], List[Tuple[str, str, float]]] = defaultdict(list)\n",
    "\n",
    "for _, row in relationships.iterrows():\n",
    "    t_i, c_i = row[\"table_i\"], row[\"column_i\"]\n",
    "    t_k, c_k = row[\"table_k\"], row[\"column_k\"]\n",
    "    s_val = float(row[\"s\"])\n",
    "\n",
    "    key1 = (t_i, c_i)\n",
    "    key2 = (t_k, c_k)\n",
    "\n",
    "    adjacency[key1].append((t_k, c_k, s_val))\n",
    "    adjacency[key2].append((t_i, c_i, s_val))\n",
    "\n",
    "len(adjacency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5101a4de-86b1-4774-a422-29e3eb32e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy cache of dataframes\n",
    "_table_cache: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "def load_table(table_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load table <data_folder>/<table_name>.csv.\n",
    "    Returns None if file is missing or unreadable.\n",
    "    \"\"\"\n",
    "    if table_name in _table_cache:\n",
    "        return _table_cache[table_name]\n",
    "\n",
    "    path = Path(data_folder) / f\"{table_name}.csv\"\n",
    "    if not path.exists():\n",
    "        print(f\"âš ï¸  Missing table file: {path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        _table_cache[table_name] = df\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Failed to read {path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41752b9a-ec74-4df8-a0af-fb88254a7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    case_id: str\n",
    "    table: str\n",
    "    time: pd.Timestamp\n",
    "    dt_column: str\n",
    "    row_index: int\n",
    "\n",
    "\n",
    "def get_dt_columns(table: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return ordered list of datetime columns for a table,\n",
    "    falling back to [] if none were identified.\n",
    "    \"\"\"\n",
    "    return dt_cols_by_table.get(table, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2a6ace-7f9a-464e-b2c0-4e3466aa34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional, Tuple, List, Set\n",
    "\n",
    "def sample_root_table() -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Choose a random table that has:\n",
    "      - at least one PK candidate\n",
    "      - at least one datetime candidate\n",
    "      - a readable CSV file\n",
    "    \"\"\"\n",
    "    valid_tables = []\n",
    "    for t in pk_cols_by_table.keys():\n",
    "        if t not in dt_cols_by_table:\n",
    "            continue\n",
    "        if load_table(t) is None:\n",
    "            continue\n",
    "        valid_tables.append(t)\n",
    "\n",
    "    if not valid_tables:\n",
    "        print(\"âŒ No tables with both PK and datetime candidates.\")\n",
    "        return None\n",
    "\n",
    "    return random.choice(valid_tables)\n",
    "\n",
    "def normalize_ts(value) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Parse any datetime-like value into a timezone-naive pandas.Timestamp.\n",
    "    - Parsed with utc=True\n",
    "    - Converted to tz-naive (UTC)\n",
    "    Returns NaT if parsing fails.\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(value, errors=\"coerce\", utc=True)\n",
    "    if pd.isna(ts):\n",
    "        return ts\n",
    "    return ts.tz_convert(None)  # drop timezone, keep as naive UTC\n",
    "\n",
    "    \n",
    "\n",
    "def sample_case_from_root(root_table: str) -> Optional[Tuple[str, List[Event]]]:\n",
    "    \"\"\"\n",
    "    Sample ONE case, using PK-centric propagation with local PK promotion:\n",
    "\n",
    "    1. Randomly select a PK candidate column from `root_table`.\n",
    "    2. Randomly select a PK value from that column.\n",
    "    3. Explore (table, pk_col, pk_val) triples via BFS:\n",
    "       - Within each visited table row, use *all* PK candidates in that row\n",
    "         to seed new triples (step 5 of your logic).\n",
    "       - Use relationship_scores adjacency to move between tables using\n",
    "         joinable columns.\n",
    "\n",
    "    Returns:\n",
    "      (case_id_label, events_sorted) or None if no events found.\n",
    "    \"\"\"\n",
    "    df_root = load_table(root_table)\n",
    "    if df_root is None:\n",
    "        return None\n",
    "\n",
    "    pk_cols = pk_cols_by_table.get(root_table, [])\n",
    "    if not pk_cols:\n",
    "        return None\n",
    "\n",
    "    # 1) Select a random PK column in the root table\n",
    "    root_pk_col = random.choice(pk_cols)\n",
    "\n",
    "    # 2) Select a random PK value from that column\n",
    "    series = df_root[root_pk_col].dropna()\n",
    "    if series.empty:\n",
    "        return None\n",
    "\n",
    "    root_value = random.choice(series.unique())\n",
    "    root_val_str = str(root_value)\n",
    "    case_id_label = f\"{root_table}.{root_pk_col}={root_value}\"\n",
    "\n",
    "    # BFS queue over (table, pk_column, pk_value_as_str)\n",
    "    queue: List[Tuple[str, str, str]] = []\n",
    "    visited_pk_triplets: Set[Tuple[str, str, str]] = set()\n",
    "    events: List[Event] = []\n",
    "\n",
    "    queue.append((root_table, root_pk_col, root_val_str))\n",
    "    visited_pk_triplets.add((root_table, root_pk_col, root_val_str))\n",
    "\n",
    "    while queue:\n",
    "        table, pk_col, pk_val_str = queue.pop(0)\n",
    "\n",
    "        df = load_table(table)\n",
    "        if df is None or pk_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # 4) Select all rows where this PK value exists\n",
    "        mask = df[pk_col].astype(str) == pk_val_str\n",
    "        matches = df[mask]\n",
    "        if matches.empty:\n",
    "            continue\n",
    "\n",
    "        dt_cols = dt_cols_by_table.get(table, [])\n",
    "        table_pk_cols = pk_cols_by_table.get(table, [])\n",
    "\n",
    "        for row_idx, row in matches.iterrows():\n",
    "            # --- Emit events from this row (temporalization) ---\n",
    "            for dt_col in dt_cols:\n",
    "                if dt_col not in df.columns:\n",
    "                    continue\n",
    "                ts = normalize_ts(row[dt_col])\n",
    "                if pd.isna(ts):\n",
    "                    continue\n",
    "                events.append(\n",
    "                    Event(\n",
    "                        case_id=case_id_label,\n",
    "                        table=table,\n",
    "                        time=ts,\n",
    "                        dt_column=dt_col,\n",
    "                        row_index=row_idx,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # --- 5) Local PK promotion: pick up *all* PKs in this row ---\n",
    "            # e.g. from Deliveries row: DLV_ID, OrderRef, maybe others\n",
    "            for local_pk_col in table_pk_cols:\n",
    "                if local_pk_col not in df.columns:\n",
    "                    continue\n",
    "                new_val = row[local_pk_col]\n",
    "                if pd.isna(new_val):\n",
    "                    continue\n",
    "                new_val_str = str(new_val)\n",
    "                triplet_local = (table, local_pk_col, new_val_str)\n",
    "                if triplet_local not in visited_pk_triplets:\n",
    "                    visited_pk_triplets.add(triplet_local)\n",
    "                    queue.append(triplet_local)\n",
    "\n",
    "            # --- 3 & 6) Use relationships to hop to connected tables ---\n",
    "            # Look at all joinable columns from this PK column\n",
    "            for (nbr_table, nbr_col, score) in adjacency.get((table, pk_col), []):\n",
    "                nbr_df = load_table(nbr_table)\n",
    "                if nbr_df is None or nbr_col not in nbr_df.columns:\n",
    "                    continue\n",
    "\n",
    "                # 4) Check if this PK value appears in the neighbor join column\n",
    "                nbr_mask = nbr_df[nbr_col].astype(str) == pk_val_str\n",
    "                nbr_matches = nbr_df[nbr_mask]\n",
    "                if nbr_matches.empty:\n",
    "                    continue\n",
    "\n",
    "                # 5) From those neighbor rows, pick up their PK candidates\n",
    "                nbr_pk_cols = pk_cols_by_table.get(nbr_table, [])\n",
    "                if not nbr_pk_cols:\n",
    "                    continue\n",
    "\n",
    "                for n_idx, n_row in nbr_matches.iterrows():\n",
    "                    # Emit neighbor events\n",
    "                    nbr_dt_cols = dt_cols_by_table.get(nbr_table, [])\n",
    "                    for dt_col in nbr_dt_cols:\n",
    "                        if dt_col not in nbr_df.columns:\n",
    "                            continue\n",
    "                        ts = normalize_ts(n_row[dt_col])\n",
    "                        if pd.isna(ts):\n",
    "                            continue\n",
    "                        events.append(\n",
    "                            Event(\n",
    "                                case_id=case_id_label,\n",
    "                                table=nbr_table,\n",
    "                                time=ts,\n",
    "                                dt_column=dt_col,\n",
    "                                row_index=n_idx,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                    # Promote neighbor PKs\n",
    "                    for nbr_pk_col in nbr_pk_cols:\n",
    "                        if nbr_pk_col not in nbr_df.columns:\n",
    "                            continue\n",
    "                        new_val = n_row[nbr_pk_col]\n",
    "                        if pd.isna(new_val):\n",
    "                            continue\n",
    "                        new_val_str = str(new_val)\n",
    "                        triplet = (nbr_table, nbr_pk_col, new_val_str)\n",
    "                        if triplet not in visited_pk_triplets:\n",
    "                            visited_pk_triplets.add(triplet)\n",
    "                            queue.append(triplet)\n",
    "\n",
    "    if not events:\n",
    "        return None\n",
    "\n",
    "    # Deduplicate repeated captures of the *same* business event\n",
    "    unique_events = {}\n",
    "    for e in events:\n",
    "        key = (e.table, e.dt_column, e.time)\n",
    "        if key not in unique_events:\n",
    "            unique_events[key] = e\n",
    "\n",
    "    events_dedup = list(unique_events.values())\n",
    "\n",
    "    # Stable time sort â†’ Îµ_s\n",
    "    events_sorted = sorted(events_dedup, key=lambda e: (e.time, e.table, e.row_index))\n",
    "    return case_id_label, events_sorted\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22dd061c-e76f-4436-9f37-d0976872ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_case_traces(\n",
    "    n_cases: int,\n",
    "    allow_duplicates: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build `n_cases` sampled traces.\n",
    "\n",
    "    Output columns:\n",
    "        case_id, root_table, root_pk_column, event_sequence, num_events\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    seen_case_ids: Set[str] = set()\n",
    "\n",
    "    while len(records) < n_cases:\n",
    "        root_table = sample_root_table()\n",
    "        if root_table is None:\n",
    "            break\n",
    "\n",
    "        result = sample_case_from_root(root_table)\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        case_id_label, events_sorted = result\n",
    "        if (not allow_duplicates) and (case_id_label in seen_case_ids):\n",
    "            continue\n",
    "        seen_case_ids.add(case_id_label)\n",
    "\n",
    "        # Full table sequence (recurrences allowed, but deduped per timestamp)\n",
    "        table_sequence = [e.table for e in events_sorted]\n",
    "        sequence_str = \" > \".join(table_sequence)\n",
    "\n",
    "        # Extract root table and root pk column from label\n",
    "        try:\n",
    "            root_info, value_str = case_id_label.split(\"=\", 1)\n",
    "            r_table, r_col = root_info.split(\".\", 1)\n",
    "        except ValueError:\n",
    "            r_table = root_table\n",
    "            r_col = pk_cols_by_table.get(root_table, [\"<unknown>\"])[0]\n",
    "\n",
    "        records.append(\n",
    "            dict(\n",
    "                case_id=case_id_label,\n",
    "                root_table=r_table,\n",
    "                root_pk_column=r_col,\n",
    "                event_sequence=sequence_str,\n",
    "                num_events=len(events_sorted),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9821b3-9b36-41b7-b8f5-822329b005e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_traces_df = build_case_traces(N_CASES_TO_SAMPLE)\n",
    "print(f\"Built {len(case_traces_df)} case traces.\")\n",
    "case_traces_df.head()\n",
    "\n",
    "case_traces_df.to_csv(traces_output_csv, index=False)\n",
    "print(f\"Case traces written to: {traces_output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
