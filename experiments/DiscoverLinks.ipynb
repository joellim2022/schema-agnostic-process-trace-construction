{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfe6126-98f2-4639-9cf0-73f37779df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7085664-f03c-4d78-a67f-5ceddc8fdb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Logical type assignment\n",
    "# -------------------------\n",
    "\n",
    "def infer_logical_type(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Map raw dtype + parse_success from profiling into a logical type:\n",
    "    'datetime', 'numeric', or 'categorical'.\n",
    "    \"\"\"\n",
    "    # If it parses well as datetime, treat as datetime\n",
    "    if row.get(\"parse_success\", 0.0) >= 0.5:\n",
    "        return \"datetime\"\n",
    "\n",
    "    dtype_str = str(row.get(\"dtype\", \"\")).lower()\n",
    "\n",
    "    # Rough numeric check\n",
    "    if any(x in dtype_str for x in [\"int\", \"float\", \"double\", \"decimal\"]):\n",
    "        return \"numeric\"\n",
    "\n",
    "    # Fallback\n",
    "    return \"categorical\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Type compatibility T\n",
    "# -------------------------\n",
    "\n",
    "TYPE_COMPAT_MATRIX: Dict[Tuple[str, str], float] = {}\n",
    "\n",
    "def _init_type_compat_matrix():\n",
    "    # Base matrix (you can tweak these weights if you like)\n",
    "    base = {\n",
    "        (\"numeric\", \"numeric\"): 1.0,\n",
    "        (\"datetime\", \"datetime\"): 1.0,\n",
    "        (\"categorical\", \"categorical\"): 1.0,\n",
    "        (\"numeric\", \"categorical\"): 0.4,\n",
    "        (\"categorical\", \"numeric\"): 0.4,\n",
    "        (\"numeric\", \"datetime\"): 0.2,\n",
    "        (\"datetime\", \"numeric\"): 0.2,\n",
    "        (\"categorical\", \"datetime\"): 0.3,\n",
    "        (\"datetime\", \"categorical\"): 0.3,\n",
    "    }\n",
    "    TYPE_COMPAT_MATRIX.update(base)\n",
    "\n",
    "_init_type_compat_matrix()\n",
    "\n",
    "\n",
    "def type_compat(t1: str, t2: str) -> float:\n",
    "    \"\"\"Soft type-compatibility T in [0,1].\"\"\"\n",
    "    return TYPE_COMPAT_MATRIX.get((t1, t2), 0.0)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Coverage factor U\n",
    "# -------------------------\n",
    "\n",
    "def coverage_factor(dist_i: int, dist_k: int, overlap: int) -> float:\n",
    "    \"\"\"\n",
    "    Coverage factor U based on distinct-value overlap.\n",
    "\n",
    "    U = min( overlap/dist_i, overlap/dist_k ), with safe guards.\n",
    "    \"\"\"\n",
    "    if overlap <= 0 or dist_i <= 0 or dist_k <= 0:\n",
    "        return 0.0\n",
    "    return float(min(overlap / dist_i, overlap / dist_k))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Value similarity R\n",
    "# -------------------------\n",
    "\n",
    "def jaccard_similarity(set_a, set_b) -> float:\n",
    "    \"\"\"Jaccard similarity for symbolic sets.\"\"\"\n",
    "    if not set_a and not set_b:\n",
    "        return 1.0\n",
    "    if not set_a or not set_b:\n",
    "        return 0.0\n",
    "    inter = len(set_a & set_b)\n",
    "    union = len(set_a | set_b)\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return inter / union\n",
    "\n",
    "\n",
    "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Jensen-Shannon divergence between two distributions p and q.\n",
    "    p and q are non-negative and sum to 1.\n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    q = np.asarray(q, dtype=float)\n",
    "    p /= p.sum() if p.sum() > 0 else 1.0\n",
    "    q /= q.sum() if q.sum() > 0 else 1.0\n",
    "\n",
    "    m = 0.5 * (p + q)\n",
    "\n",
    "    def kl_div(a, b):\n",
    "        mask = (a > 0) & (b > 0)\n",
    "        a = a[mask]\n",
    "        b = b[mask]\n",
    "        return np.sum(a * np.log2(a / b))\n",
    "\n",
    "    return 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)\n",
    "\n",
    "\n",
    "def ks_statistic(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Simple two-sample Kolmogorov-Smirnov statistic D in [0,1].\n",
    "    \"\"\"\n",
    "    a = np.sort(a)\n",
    "    b = np.sort(b)\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0 or m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    i = j = 0\n",
    "    d = 0.0\n",
    "    while i < n and j < m:\n",
    "        if a[i] <= b[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "        cdf_a = i / n\n",
    "        cdf_b = j / m\n",
    "        d = max(d, abs(cdf_a - cdf_b))\n",
    "    return d\n",
    "\n",
    "\n",
    "def numeric_similarity(arr_i: np.ndarray, arr_k: np.ndarray, n_bins: int = 20) -> float:\n",
    "    \"\"\"\n",
    "    JS/KS blend for numeric-numeric columns.\n",
    "\n",
    "    Returns R in [0,1].\n",
    "    \"\"\"\n",
    "    arr_i = np.asarray(arr_i, dtype=float)\n",
    "    arr_k = np.asarray(arr_k, dtype=float)\n",
    "\n",
    "    if len(arr_i) == 0 or len(arr_k) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Optional down-sampling for speed on huge arrays\n",
    "    max_sample = 10000\n",
    "    if len(arr_i) > max_sample:\n",
    "        arr_i = np.random.choice(arr_i, size=max_sample, replace=False)\n",
    "    if len(arr_k) > max_sample:\n",
    "        arr_k = np.random.choice(arr_k, size=max_sample, replace=False)\n",
    "\n",
    "    # Common bin edges\n",
    "    lo = min(arr_i.min(), arr_k.min())\n",
    "    hi = max(arr_i.max(), arr_k.max())\n",
    "    if lo == hi:\n",
    "        return 1.0  # identical constant columns\n",
    "\n",
    "    hist_i, bins = np.histogram(arr_i, bins=n_bins, range=(lo, hi), density=False)\n",
    "    hist_k, _ = np.histogram(arr_k, bins=bins, density=False)\n",
    "\n",
    "    # JS divergence -> similarity\n",
    "    js = js_divergence(hist_i, hist_k)\n",
    "    js = min(1.0, max(0.0, js))\n",
    "    js_sim = 1.0 - js\n",
    "\n",
    "    # KS statistic -> similarity\n",
    "    ks = ks_statistic(arr_i, arr_k)\n",
    "    ks = min(1.0, max(0.0, ks))\n",
    "    ks_sim = 1.0 - ks\n",
    "\n",
    "    return 0.5 * (js_sim + ks_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8058bdb2-d338-4f6c-888c-c56da8d6488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ColumnMeta:\n",
    "    table: str\n",
    "    column: str\n",
    "    logical_type: str\n",
    "    is_pk_candidate: bool\n",
    "    is_dt_candidate: bool\n",
    "    completeness: float\n",
    "    distinctness: float\n",
    "    distinct_count: int\n",
    "\n",
    "\n",
    "def build_column_meta(df_profiles: pd.DataFrame) -> List[ColumnMeta]:\n",
    "    meta_list: List[ColumnMeta] = []\n",
    "    for _, row in df_profiles.iterrows():\n",
    "        logical = infer_logical_type(row)\n",
    "        meta_list.append(\n",
    "            ColumnMeta(\n",
    "                table=row[\"table\"],\n",
    "                column=row[\"column\"],\n",
    "                logical_type=logical,\n",
    "                is_pk_candidate=bool(row.get(\"is_pk_candidate\", False)),\n",
    "                is_dt_candidate=bool(row.get(\"is_dt_candidate\", False)),\n",
    "                completeness=float(row.get(\"completeness\", 0.0)),\n",
    "                distinctness=float(row.get(\"distinctness\", 0.0)),\n",
    "                distinct_count=int(row.get(\"distinct_count\", 0)),\n",
    "            )\n",
    "        )\n",
    "    return meta_list\n",
    "\n",
    "\n",
    "def load_tables(data_folder: str, table_names: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load each table from <data_folder>/<table>.csv\n",
    "    (assuming file stem matches the 'table' column from profiling).\n",
    "    \"\"\"\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "    base = Path(data_folder)\n",
    "\n",
    "    for t in sorted(set(table_names)):\n",
    "        path = base / f\"{t}.csv\"\n",
    "        if not path.exists():\n",
    "            print(f\"âš ï¸  Table file not found: {path}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            dfs[t] = df\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to read {path}: {e}\")\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def compute_relationship_scores(\n",
    "    df_profiles: pd.DataFrame,\n",
    "    data_folder: str,\n",
    "    min_score: float = 0.0,\n",
    "    require_pk_in_pair: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to compute s(c_i, c_k) = T * Q * U * R for column pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_profiles : DataFrame\n",
    "        Output from Section 3.1 (profiling).\n",
    "    data_folder : str\n",
    "        Folder containing the raw table CSVs.\n",
    "    min_score : float\n",
    "        Only keep pairs with s >= min_score.\n",
    "    require_pk_in_pair : bool\n",
    "        If True, only consider pairs where at least one column is a PK candidate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with one row per column pair and columns:\n",
    "        - table_i, column_i, role_i, logical_type_i, ...\n",
    "        - table_k, column_k, role_k, logical_type_k, ...\n",
    "        - T, Q, U, R, overlap_count, s\n",
    "    \"\"\"\n",
    "    # Build column metadata (primary key & datetime flags)\n",
    "    meta_list = build_column_meta(df_profiles)\n",
    "\n",
    "    # Load only tables that actually appear in profiling\n",
    "    table_names = [m.table for m in meta_list]\n",
    "    table_to_df = load_tables(data_folder, table_names)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # Pre-lookup: group metadata by table for speed\n",
    "    by_table: Dict[str, List[ColumnMeta]] = {}\n",
    "    for m in meta_list:\n",
    "        by_table.setdefault(m.table, []).append(m)\n",
    "\n",
    "    # Pre-calc mapping for (table, column) -> profile row (for completeness/distinctness)\n",
    "    prof_index = df_profiles.set_index([\"table\", \"column\"])\n",
    "\n",
    "    # Iterate over cross-table pairs\n",
    "    tables = sorted(by_table.keys())\n",
    "    for i_idx, t_i in enumerate(tables):\n",
    "        for t_k in tables[i_idx + 1 :]:\n",
    "            df_i = table_to_df.get(t_i)\n",
    "            df_k = table_to_df.get(t_k)\n",
    "            if df_i is None or df_k is None:\n",
    "                continue\n",
    "\n",
    "            cols_i = by_table[t_i]\n",
    "            cols_k = by_table[t_k]\n",
    "\n",
    "            for meta_i in cols_i:\n",
    "                for meta_k in cols_k:\n",
    "                    # Optional gate: at least one column must be a PK candidate\n",
    "                    if require_pk_in_pair and not (meta_i.is_pk_candidate or meta_k.is_pk_candidate):\n",
    "                        continue\n",
    "\n",
    "                    # ---------------------------\n",
    "                    # T: type compatibility\n",
    "                    # ---------------------------\n",
    "                    T_val = type_compat(meta_i.logical_type, meta_k.logical_type)\n",
    "                    if T_val <= 0.0:\n",
    "                        continue  # incompatible types, skip\n",
    "\n",
    "                    # Q term: conservative column quality\n",
    "                    qi = meta_i.completeness * meta_i.distinctness\n",
    "                    qk = meta_k.completeness * meta_k.distinctness\n",
    "                    Q_val = min(qi, qk)\n",
    "\n",
    "                    # Get the raw Series for values\n",
    "                    if meta_i.column not in df_i.columns or meta_k.column not in df_k.columns:\n",
    "                        continue\n",
    "\n",
    "                    s_i = df_i[meta_i.column].dropna()\n",
    "                    s_k = df_k[meta_k.column].dropna()\n",
    "\n",
    "                    # Evaluate distinct-value overlap sets\n",
    "                    # Use strings for generality\n",
    "                    set_i = set(map(str, s_i.unique()))\n",
    "                    set_k = set(map(str, s_k.unique()))\n",
    "                    overlap_count = len(set_i & set_k)\n",
    "\n",
    "                    U_val = coverage_factor(meta_i.distinct_count, meta_k.distinct_count, overlap_count)\n",
    "                    if overlap_count == 0 or U_val == 0.0:\n",
    "                        continue  # no overlap, no relationship\n",
    "\n",
    "                    # ---------------------------\n",
    "                    # R: value similarity\n",
    "                    # ---------------------------\n",
    "                    if meta_i.logical_type == \"numeric\" and meta_k.logical_type == \"numeric\":\n",
    "                        arr_i = pd.to_numeric(s_i, errors=\"coerce\").dropna().to_numpy()\n",
    "                        arr_k = pd.to_numeric(s_k, errors=\"coerce\").dropna().to_numpy()\n",
    "                        R_val = numeric_similarity(arr_i, arr_k)\n",
    "                    else:\n",
    "                        # Symbolic / mixed types -> Jaccard on strings\n",
    "                        R_val = jaccard_similarity(set_i, set_k)\n",
    "\n",
    "                    # Final score\n",
    "                    s_val = T_val * Q_val * U_val * R_val\n",
    "\n",
    "                    if s_val < min_score:\n",
    "                        continue\n",
    "\n",
    "                    role_i = \";\".join(\n",
    "                        r for r, flag in [\n",
    "                            (\"pk\", meta_i.is_pk_candidate),\n",
    "                            (\"dt\", meta_i.is_dt_candidate),\n",
    "                        ]\n",
    "                        if flag\n",
    "                    ) or \"other\"\n",
    "\n",
    "                    role_k = \";\".join(\n",
    "                        r for r, flag in [\n",
    "                            (\"pk\", meta_k.is_pk_candidate),\n",
    "                            (\"dt\", meta_k.is_dt_candidate),\n",
    "                        ]\n",
    "                        if flag\n",
    "                    ) or \"other\"\n",
    "\n",
    "                    records.append(\n",
    "                        dict(\n",
    "                            table_i=meta_i.table,\n",
    "                            column_i=meta_i.column,\n",
    "                            logical_type_i=meta_i.logical_type,\n",
    "                            role_i=role_i,\n",
    "                            completeness_i=meta_i.completeness,\n",
    "                            distinctness_i=meta_i.distinctness,\n",
    "                            distinct_count_i=meta_i.distinct_count,\n",
    "                            table_k=meta_k.table,\n",
    "                            column_k=meta_k.column,\n",
    "                            logical_type_k=meta_k.logical_type,\n",
    "                            role_k=role_k,\n",
    "                            completeness_k=meta_k.completeness,\n",
    "                            distinctness_k=meta_k.distinctness,\n",
    "                            distinct_count_k=meta_k.distinct_count,\n",
    "                            T=T_val,\n",
    "                            Q=Q_val,\n",
    "                            U=U_val,\n",
    "                            R=R_val,\n",
    "                            overlap_count=overlap_count,\n",
    "                            s=s_val,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    rel_df = pd.DataFrame.from_records(records)\n",
    "    return rel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb546b14-6554-4558-9714-66fd5f934420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 16 candidate relationships.\n",
      "Relationship scores written to: C:\\Users\\joel\\Desktop\\CAiSE_2\\DGP_1\\data\\relationship_scores.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_i</th>\n",
       "      <th>column_i</th>\n",
       "      <th>logical_type_i</th>\n",
       "      <th>role_i</th>\n",
       "      <th>completeness_i</th>\n",
       "      <th>distinctness_i</th>\n",
       "      <th>distinct_count_i</th>\n",
       "      <th>table_k</th>\n",
       "      <th>column_k</th>\n",
       "      <th>logical_type_k</th>\n",
       "      <th>role_k</th>\n",
       "      <th>completeness_k</th>\n",
       "      <th>distinctness_k</th>\n",
       "      <th>distinct_count_k</th>\n",
       "      <th>T</th>\n",
       "      <th>Q</th>\n",
       "      <th>U</th>\n",
       "      <th>R</th>\n",
       "      <th>overlap_count</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancellations</td>\n",
       "      <td>OrigOrderKey</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3661</td>\n",
       "      <td>Orders</td>\n",
       "      <td>ORD_ID</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9155</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.399891</td>\n",
       "      <td>0.399891</td>\n",
       "      <td>3661</td>\n",
       "      <td>0.159913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CardUpdates</td>\n",
       "      <td>CustomerAlias</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>CardVerifications</td>\n",
       "      <td>CustJoinKey</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CardUpdates</td>\n",
       "      <td>CustomerAlias</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>CustomerValidation</td>\n",
       "      <td>CustKey</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CardUpdates</td>\n",
       "      <td>CustomerAlias</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>Customers</td>\n",
       "      <td>CustomerID</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CardUpdates</td>\n",
       "      <td>CustomerAlias</td>\n",
       "      <td>categorical</td>\n",
       "      <td>pk</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>Orders</td>\n",
       "      <td>CustRef</td>\n",
       "      <td>categorical</td>\n",
       "      <td>other</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308247</td>\n",
       "      <td>2822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308247</td>\n",
       "      <td>0.940667</td>\n",
       "      <td>0.940667</td>\n",
       "      <td>2822</td>\n",
       "      <td>0.272753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         table_i       column_i logical_type_i role_i  completeness_i  \\\n",
       "0  Cancellations   OrigOrderKey    categorical     pk             1.0   \n",
       "1    CardUpdates  CustomerAlias    categorical     pk             1.0   \n",
       "2    CardUpdates  CustomerAlias    categorical     pk             1.0   \n",
       "3    CardUpdates  CustomerAlias    categorical     pk             1.0   \n",
       "4    CardUpdates  CustomerAlias    categorical     pk             1.0   \n",
       "\n",
       "   distinctness_i  distinct_count_i             table_k     column_k  \\\n",
       "0             1.0              3661              Orders       ORD_ID   \n",
       "1             1.0              3000   CardVerifications  CustJoinKey   \n",
       "2             1.0              3000  CustomerValidation      CustKey   \n",
       "3             1.0              3000           Customers   CustomerID   \n",
       "4             1.0              3000              Orders      CustRef   \n",
       "\n",
       "  logical_type_k role_k  completeness_k  distinctness_k  distinct_count_k  \\\n",
       "0    categorical     pk             1.0        1.000000              9155   \n",
       "1    categorical     pk             1.0        1.000000              3000   \n",
       "2    categorical     pk             1.0        1.000000              3000   \n",
       "3    categorical     pk             1.0        1.000000              3000   \n",
       "4    categorical  other             1.0        0.308247              2822   \n",
       "\n",
       "     T         Q         U         R  overlap_count         s  \n",
       "0  1.0  1.000000  0.399891  0.399891           3661  0.159913  \n",
       "1  1.0  1.000000  1.000000  1.000000           3000  1.000000  \n",
       "2  1.0  1.000000  1.000000  1.000000           3000  1.000000  \n",
       "3  1.0  1.000000  1.000000  1.000000           3000  1.000000  \n",
       "4  1.0  0.308247  0.940667  0.940667           2822  0.272753  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === USER INPUTS ===\n",
    "# Path to profiling output from Section 3.1 (s_pk, s_dt, etc.)\n",
    "profiling_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\profile_primitives.csv\"   # ðŸ‘ˆ change this\n",
    "\n",
    "# Folder containing the raw table CSVs\n",
    "data_folder = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\food_deliver_clean\"                                  # ðŸ‘ˆ change this\n",
    "\n",
    "# Where to write the relationship scores CSV\n",
    "relationship_output_csv = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\DGP_1\\\\data\\\\relationship_scores.csv\"   # ðŸ‘ˆ change this\n",
    "\n",
    "\n",
    "\n",
    "# # Path to profiling output from Section 3.1 (s_pk, s_dt, etc.)\n",
    "# profiling_csv_path = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\ProfilePrimitives_TPCH.csv\"   # ðŸ‘ˆ change this\n",
    "\n",
    "# # Folder containing the raw table CSVs\n",
    "# data_folder = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\tpch_csv\"                                  # ðŸ‘ˆ change this\n",
    "\n",
    "# # Where to write the relationship scores CSV\n",
    "# relationship_output_csv = \"C:\\\\Users\\\\joel\\\\Desktop\\\\CAiSE_2\\\\TPC_H\\\\relationship_scores.csv\"   # ðŸ‘ˆ change this\n",
    "\n",
    "\n",
    "\n",
    "# Optional gates\n",
    "MIN_SCORE = 0.0        # keep everything with s >= MIN_SCORE\n",
    "REQUIRE_PK_IN_PAIR = True  # set False if you want all candidate pairs\n",
    "\n",
    "# === RUN ===\n",
    "df_profiles = pd.read_csv(profiling_csv_path)\n",
    "\n",
    "relationship_df = compute_relationship_scores(\n",
    "    df_profiles=df_profiles,\n",
    "    data_folder=data_folder,\n",
    "    min_score=MIN_SCORE,\n",
    "    require_pk_in_pair=REQUIRE_PK_IN_PAIR,\n",
    ")\n",
    "\n",
    "print(f\"Computed {len(relationship_df)} candidate relationships.\")\n",
    "relationship_df.to_csv(relationship_output_csv, index=False)\n",
    "print(f\"Relationship scores written to: {relationship_output_csv}\")\n",
    "\n",
    "relationship_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
