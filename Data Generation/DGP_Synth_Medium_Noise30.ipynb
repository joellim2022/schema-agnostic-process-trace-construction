{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e41fd4-d649-4149-b378-2051f28830a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50000 traces and wrote tables to 'data'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# CONFIG\n",
    "# ================================================================\n",
    "\n",
    "MEDIUM_N_TRACES = 10_000\n",
    "RANDOM_SEED = 123\n",
    "DATA_DIR_MEDIUM = \"./data_medium\"\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# TRANSITION GRAPH (EVENT-LEVEL)\n",
    "# ================================================================\n",
    "\n",
    "def build_medium_transition_graph() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Defines allowed transitions between events, including the two\n",
    "    branching points:\n",
    "      - RiskCheckCompleted -> {AutoApprovalGranted, ManualReviewRequired}\n",
    "      - DeliveryExceptionOccurred -> {ReroutePlanned, DelayLogged,\n",
    "                                      DriverReassigned, OrderCancelled}\n",
    "    \"\"\"\n",
    "    transitions = {\n",
    "        \"CustomerCreated\": [\"AddressAdded\"],\n",
    "        \"AddressAdded\": [\"DeviceRegistered\"],\n",
    "        \"DeviceRegistered\": [\"MerchantCreated\"],\n",
    "        \"MerchantCreated\": [\"LocationActivated\"],\n",
    "        \"LocationActivated\": [\"OrderCreated\"],\n",
    "        \"OrderCreated\": [\"CartSnapshotSaved\"],\n",
    "        \"CartSnapshotSaved\": [\"ExperimentBucketAssigned\"],\n",
    "        \"ExperimentBucketAssigned\": [\"PromoValidated\"],\n",
    "        \"PromoValidated\": [\"TaxCalculated\"],\n",
    "        \"TaxCalculated\": [\"OrderConfirmed\"],\n",
    "        \"OrderConfirmed\": [\"RiskCheckStarted\"],\n",
    "        \"RiskCheckStarted\": [\"RiskScoreCalculated\"],\n",
    "        \"RiskScoreCalculated\": [\"FraudCheckPerformed\"],\n",
    "        \"FraudCheckPerformed\": [\"RiskCheckCompleted\"],\n",
    "        # Branch 1 (2-way)\n",
    "        \"RiskCheckCompleted\": [\"AutoApprovalGranted\", \"ManualReviewRequired\"],\n",
    "        \"AutoApprovalGranted\": [\"KitchenTicketCreated\"],\n",
    "        \"ManualReviewRequired\": [\"ManualReviewCompleted\"],\n",
    "        \"ManualReviewCompleted\": [\"KitchenTicketCreated\"],\n",
    "        \"KitchenTicketCreated\": [\"KitchenCookingStarted\"],\n",
    "        \"KitchenCookingStarted\": [\"KitchenCookingFinished\"],\n",
    "        \"KitchenCookingFinished\": [\"DispatchRequested\"],\n",
    "        \"DispatchRequested\": [\"DriverShiftOnline\"],\n",
    "        \"DriverShiftOnline\": [\"DriverAssigned\"],\n",
    "        \"DriverAssigned\": [\"RoutePlanned\"],\n",
    "        \"RoutePlanned\": [\"SegmentStarted\"],\n",
    "        \"SegmentStarted\": [\"SegmentCompleted\"],\n",
    "        \"SegmentCompleted\": [\"Delivered\"],\n",
    "        \"Delivered\": [\"PaymentAuthorized\", \"DeliveryExceptionOccurred\"],\n",
    "        # Branch 2 (4-way)\n",
    "        \"DeliveryExceptionOccurred\": [\n",
    "            \"ReroutePlanned\",\n",
    "            \"DelayLogged\",\n",
    "            \"DriverReassigned\",\n",
    "            \"OrderCancelled\",\n",
    "        ],\n",
    "        \"ReroutePlanned\": [\"SegmentStarted\"],\n",
    "        \"DelayLogged\": [\"PaymentAuthorized\"],\n",
    "        \"DriverReassigned\": [\"RoutePlanned\"],\n",
    "        \"OrderCancelled\": [],\n",
    "        \"PaymentAuthorized\": [\"PaymentCaptured\"],\n",
    "        \"PaymentCaptured\": [\"PaymentSettled\"],\n",
    "        \"PaymentSettled\": [\"SupportTicketOpened\", \"ReviewSubmitted\"],\n",
    "        \"SupportTicketOpened\": [\"SupportMessageAdded\"],\n",
    "        \"SupportMessageAdded\": [\"SupportIssueResolved\"],\n",
    "        \"SupportIssueResolved\": [\"ReviewSubmitted\"],\n",
    "        \"ReviewSubmitted\": [\"RewardCredited\", \"AuditLogWritten\"],\n",
    "        \"RewardCredited\": [\"AuditLogWritten\"],\n",
    "        \"AuditLogWritten\": [],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for src, dst_list in transitions.items():\n",
    "        rows.append({\"From\": src, \"To_List\": str(dst_list)})\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"From\", \"To_List\"])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# HELPER: TIMESTAMP ADVANCER\n",
    "# ================================================================\n",
    "\n",
    "def make_ts_advancer(base_ts: pd.Timestamp):\n",
    "    \"\"\"\n",
    "    Returns a closure next_ts(delta_minutes) that always moves forward.\n",
    "    \"\"\"\n",
    "    current_ts = base_ts\n",
    "\n",
    "    def next_ts(delta_minutes: int) -> pd.Timestamp:\n",
    "        nonlocal current_ts\n",
    "        current_ts = current_ts + pd.Timedelta(minutes=delta_minutes)\n",
    "        return current_ts\n",
    "\n",
    "    return next_ts\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1. CUSTOMER-RELATED TABLES\n",
    "# ================================================================\n",
    "\n",
    "def generate_customers_medium(n_customers: int = 3000) -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "]:\n",
    "    customer_ids = np.arange(1, n_customers + 1)\n",
    "\n",
    "    base_dates = pd.Timestamp(\"2024-01-01\") + pd.to_timedelta(\n",
    "        rng.integers(0, 90, size=n_customers), unit=\"D\"\n",
    "    )\n",
    "    created_ts = base_dates\n",
    "    updated_ts = created_ts + pd.to_timedelta(\n",
    "        rng.integers(1, 10, size=n_customers), unit=\"D\"\n",
    "    )\n",
    "\n",
    "    customers = pd.DataFrame({\n",
    "        \"customer_id\": customer_ids,\n",
    "        \"name\": [f\"Customer_{i}\" for i in customer_ids],\n",
    "        \"region\": rng.choice([\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"], size=n_customers),\n",
    "        \"credit_rating\": rng.integers(1, 6, size=n_customers),\n",
    "        \"outstanding_amount\": rng.uniform(0, 500, size=n_customers).round(2),\n",
    "        \"created_ts\": created_ts,\n",
    "        \"updated_ts\": updated_ts,\n",
    "    })\n",
    "\n",
    "    # Addresses\n",
    "    addr_rows = []\n",
    "    addr_id = 1\n",
    "    for idx, row in customers.iterrows():\n",
    "        cid = row[\"customer_id\"]\n",
    "        reg = row[\"region\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        n_addr = rng.integers(1, 4)\n",
    "        for _ in range(n_addr):\n",
    "            addr_rows.append({\n",
    "                \"address_id\": addr_id,\n",
    "                \"cust_ref\": cid,\n",
    "                \"address_line\": f\"{addr_id} Main Street\",\n",
    "                \"zone\": reg,\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 30))),\n",
    "                \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(5, 40))),\n",
    "            })\n",
    "            addr_id += 1\n",
    "    customer_addresses = pd.DataFrame(addr_rows)\n",
    "\n",
    "    # Devices\n",
    "    dev_rows = []\n",
    "    dev_id = 1\n",
    "    for idx, row in customers.iterrows():\n",
    "        cid = row[\"customer_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        n_dev = rng.integers(1, 3)\n",
    "        for _ in range(n_dev):\n",
    "            dev_rows.append({\n",
    "                \"device_id\": dev_id,\n",
    "                \"cust_ref\": cid,\n",
    "                \"device_type\": rng.choice([\"IOS\", \"ANDROID\", \"WEB\"]),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 20))),\n",
    "                \"last_seen_ts\": base + pd.Timedelta(days=int(rng.integers(10, 60))),\n",
    "            })\n",
    "            dev_id += 1\n",
    "    customer_devices = pd.DataFrame(dev_rows)\n",
    "\n",
    "    # Payment methods\n",
    "    pay_rows = []\n",
    "    pay_id = 1\n",
    "    for idx, row in customers.iterrows():\n",
    "        cid = row[\"customer_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        n_methods = rng.integers(1, 4)\n",
    "        for _ in range(n_methods):\n",
    "            mtype = rng.choice([\"CARD\", \"WALLET\", \"BANK\"])\n",
    "            wallet_balance = (\n",
    "                rng.uniform(0, 300) if mtype == \"WALLET\" else rng.uniform(20, 300)\n",
    "            )\n",
    "            pay_rows.append({\n",
    "                \"pay_method_id\": pay_id,\n",
    "                \"cust_ref\": cid,\n",
    "                \"method_type\": mtype,\n",
    "                \"wallet_balance\": round(float(wallet_balance), 2),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 10))),\n",
    "                \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(5, 30))),\n",
    "            })\n",
    "            pay_id += 1\n",
    "    customer_payment_methods = pd.DataFrame(pay_rows)\n",
    "\n",
    "    # Verification (KYC)\n",
    "    ver_rows = []\n",
    "    for idx, row in customers.iterrows():\n",
    "        cid = row[\"customer_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        submitted_ts = base + pd.Timedelta(days=int(rng.integers(0, 10)))\n",
    "        verified_ts = submitted_ts + pd.Timedelta(days=int(rng.integers(1, 5)))\n",
    "        ver_rows.append({\n",
    "            \"kyc_id\": cid,  # one row per customer\n",
    "            \"cust_ref\": cid,\n",
    "            \"status\": \"VERIFIED\",\n",
    "            \"submitted_ts\": submitted_ts,\n",
    "            \"verified_ts\": verified_ts,\n",
    "            \"updated_ts\": verified_ts + pd.Timedelta(days=1),\n",
    "        })\n",
    "    customer_verification = pd.DataFrame(ver_rows)\n",
    "\n",
    "    # Loyalty\n",
    "    loy_rows = []\n",
    "    for idx, row in customers.iterrows():\n",
    "        cid = row[\"customer_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        loy_rows.append({\n",
    "            \"loyalty_id\": cid,\n",
    "            \"cust_ref\": cid,\n",
    "            \"tier\": rng.choice([\"BRONZE\", \"SILVER\", \"GOLD\", \"PLATINUM\"]),\n",
    "            \"effective_ts\": base + pd.Timedelta(days=int(rng.integers(5, 40))),\n",
    "            \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(30, 80))),\n",
    "        })\n",
    "    customer_loyalty_status = pd.DataFrame(loy_rows)\n",
    "\n",
    "    return (\n",
    "        customers,\n",
    "        customer_addresses,\n",
    "        customer_devices,\n",
    "        customer_payment_methods,\n",
    "        customer_verification,\n",
    "        customer_loyalty_status,\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2. MERCHANT / MENU TABLES\n",
    "# ================================================================\n",
    "\n",
    "def generate_merchants_and_menu_medium(\n",
    "    n_merchants: int = 300,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "]:\n",
    "    merch_ids = np.arange(1, n_merchants + 1)\n",
    "    base_dates = pd.Timestamp(\"2024-01-01\") + pd.to_timedelta(\n",
    "        rng.integers(0, 90, size=n_merchants), unit=\"D\"\n",
    "    )\n",
    "\n",
    "    merchants = pd.DataFrame({\n",
    "        \"merchant_id\": merch_ids,\n",
    "        \"name\": [f\"Merchant_{i}\" for i in merch_ids],\n",
    "        \"category\": rng.choice([\"RESTAURANT\", \"GROCERY\", \"LIQUOR\"], size=n_merchants),\n",
    "        \"created_ts\": base_dates,\n",
    "        \"updated_ts\": base_dates + pd.to_timedelta(\n",
    "            rng.integers(5, 40, size=n_merchants), unit=\"D\"\n",
    "        ),\n",
    "    })\n",
    "\n",
    "    # Locations\n",
    "    loc_rows = []\n",
    "    loc_id = 1\n",
    "    for idx, row in merchants.iterrows():\n",
    "        mid = row[\"merchant_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        n_locs = rng.integers(1, 4)\n",
    "        for _ in range(n_locs):\n",
    "            loc_rows.append({\n",
    "                \"location_id\": loc_id,\n",
    "                \"merchant_ref\": mid,\n",
    "                \"address_line\": f\"MerchantLoc_{loc_id}\",\n",
    "                \"region\": rng.choice([\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"]),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 20))),\n",
    "                \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(10, 60))),\n",
    "            })\n",
    "            loc_id += 1\n",
    "    merchant_locations = pd.DataFrame(loc_rows)\n",
    "\n",
    "    # Business docs\n",
    "    doc_rows = []\n",
    "    doc_id = 1\n",
    "    for idx, row in merchants.iterrows():\n",
    "        mid = row[\"merchant_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        submitted = base + pd.Timedelta(days=int(rng.integers(0, 15)))\n",
    "        verified = submitted + pd.Timedelta(days=int(rng.integers(1, 10)))\n",
    "        doc_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"merchant_ref\": mid,\n",
    "            \"submitted_ts\": submitted,\n",
    "            \"verified_ts\": verified,\n",
    "            \"updated_ts\": verified + pd.Timedelta(days=1),\n",
    "        })\n",
    "        doc_id += 1\n",
    "    merchant_business_docs = pd.DataFrame(doc_rows)\n",
    "\n",
    "    # Bank accounts\n",
    "    bank_rows = []\n",
    "    bank_id = 1\n",
    "    for idx, row in merchants.iterrows():\n",
    "        mid = row[\"merchant_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        bank_rows.append({\n",
    "            \"bank_id\": bank_id,\n",
    "            \"merchant_ref\": mid,\n",
    "            \"account_number\": f\"ACCT_{bank_id:05d}\",\n",
    "            \"linked_ts\": base + pd.Timedelta(days=int(rng.integers(1, 20))),\n",
    "            \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(20, 60))),\n",
    "        })\n",
    "        bank_id += 1\n",
    "    merchant_bank_accounts = pd.DataFrame(bank_rows)\n",
    "\n",
    "    # Operational status\n",
    "    op_rows = []\n",
    "    op_id = 1\n",
    "    for idx, row in merchants.iterrows():\n",
    "        mid = row[\"merchant_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        op_rows.append({\n",
    "            \"status_id\": op_id,\n",
    "            \"merchant_ref\": mid,\n",
    "            \"status\": rng.choice([\"ACTIVE\", \"INACTIVE\"], p=[0.9, 0.1]),\n",
    "            \"effective_ts\": base + pd.Timedelta(days=int(rng.integers(1, 10))),\n",
    "            \"batch_run_ts\": base + pd.Timedelta(days=int(rng.integers(20, 80))),\n",
    "        })\n",
    "        op_id += 1\n",
    "    merchant_operational_status = pd.DataFrame(op_rows)\n",
    "\n",
    "    # Menu items + options + inventory + updates + inventory reservations\n",
    "    menu_rows = []\n",
    "    opt_rows = []\n",
    "    inv_rows = []\n",
    "    adj_rows = []\n",
    "    upd_rows = []\n",
    "    inv_reserve_rows = []\n",
    "\n",
    "    item_id = 1\n",
    "    opt_id = 1\n",
    "    inv_id = 1\n",
    "    adj_id = 1\n",
    "    upd_id = 1\n",
    "    inv_res_id = 1\n",
    "\n",
    "    categories = [\"FOOD\", \"DRINK\", \"DESSERT\", \"GROCERY\"]\n",
    "\n",
    "    for _, loc_row in merchant_locations.iterrows():\n",
    "        base = loc_row[\"created_ts\"]\n",
    "        loc = loc_row[\"location_id\"]\n",
    "        n_items = rng.integers(5, 15)\n",
    "\n",
    "        for _ in range(n_items):\n",
    "            cat = rng.choice(categories)\n",
    "            price = float(rng.uniform(5, 60))\n",
    "\n",
    "            menu_rows.append({\n",
    "                \"item_id\": item_id,\n",
    "                \"location_ref\": loc,\n",
    "                \"name\": f\"Item_{item_id}\",\n",
    "                \"category\": cat,\n",
    "                \"base_price\": round(price, 2),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 10))),\n",
    "                \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(10, 40))),\n",
    "            })\n",
    "\n",
    "            # Options\n",
    "            n_opts = rng.integers(0, 4)\n",
    "            for _ in range(n_opts):\n",
    "                opt_rows.append({\n",
    "                    \"option_id\": opt_id,\n",
    "                    \"menu_ref\": item_id,\n",
    "                    \"option_name\": rng.choice([\"LARGE\", \"SPICY\", \"ADD_ON\", \"SIZE_UP\"]),\n",
    "                    \"extra_price\": round(float(rng.uniform(0.5, 8)), 2),\n",
    "                    \"created_ts\": base + pd.Timedelta(days=int(rng.integers(0, 10))),\n",
    "                    \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(10, 40))),\n",
    "                })\n",
    "                opt_id += 1\n",
    "\n",
    "            # Inventory snapshot\n",
    "            inv_rows.append({\n",
    "                \"inventory_id\": inv_id,\n",
    "                \"menu_ref\": item_id,\n",
    "                \"stock_level\": int(rng.integers(0, 500)),\n",
    "                \"snapshot_ts\": base + pd.Timedelta(days=int(rng.integers(0, 5))),\n",
    "                \"updated_ts\": base + pd.Timedelta(days=int(rng.integers(5, 30))),\n",
    "            })\n",
    "            inv_id += 1\n",
    "\n",
    "            # Inventory adjustment\n",
    "            adj_rows.append({\n",
    "                \"adjustment_id\": adj_id,\n",
    "                \"menu_ref\": item_id,\n",
    "                \"delta\": int(rng.integers(-10, 10)),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(5, 25))),\n",
    "                \"batch_run_ts\": base + pd.Timedelta(days=int(rng.integers(25, 60))),\n",
    "            })\n",
    "            adj_id += 1\n",
    "\n",
    "            # Menu updates\n",
    "            upd_rows.append({\n",
    "                \"update_id\": upd_id,\n",
    "                \"menu_ref\": item_id,\n",
    "                \"update_type\": rng.choice([\"PRICE_CHANGE\", \"DESC_CHANGE\", \"IMAGE_CHANGE\"]),\n",
    "                \"created_ts\": base + pd.Timedelta(days=int(rng.integers(2, 30))),\n",
    "                \"applied_ts\": base + pd.Timedelta(days=int(rng.integers(5, 40))),\n",
    "            })\n",
    "            upd_id += 1\n",
    "\n",
    "            # Inventory reservations (pseudo-dim)\n",
    "            inv_reserve_rows.append({\n",
    "                \"reservation_id\": inv_res_id,\n",
    "                \"menu_ref\": item_id,\n",
    "                \"reserved_qty\": int(rng.integers(0, 20)),\n",
    "                \"reserved_ts\": base + pd.Timedelta(days=int(rng.integers(1, 20))),\n",
    "            })\n",
    "            inv_res_id += 1\n",
    "\n",
    "            item_id += 1\n",
    "\n",
    "    menu_items = pd.DataFrame(menu_rows)\n",
    "    menu_item_options = pd.DataFrame(opt_rows)\n",
    "    inventory = pd.DataFrame(inv_rows)\n",
    "    inventory_adjustments = pd.DataFrame(adj_rows)\n",
    "    merchant_menu_updates = pd.DataFrame(upd_rows)\n",
    "    inventory_reservations = pd.DataFrame(inv_reserve_rows)\n",
    "\n",
    "    return (\n",
    "        merchants,\n",
    "        merchant_locations,\n",
    "        merchant_business_docs,\n",
    "        merchant_bank_accounts,\n",
    "        merchant_operational_status,\n",
    "        menu_items,\n",
    "        menu_item_options,\n",
    "        inventory_reservations,\n",
    "        inventory_adjustments,\n",
    "        merchant_menu_updates,\n",
    "    )\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3. DRIVER / LOGISTICS TABLES\n",
    "# ================================================================\n",
    "\n",
    "def generate_drivers_and_logistics_medium(\n",
    "    n_drivers: int = 1000,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    driver_ids = np.arange(1, n_drivers + 1)\n",
    "    base_dates = pd.Timestamp(\"2024-02-01\") + pd.to_timedelta(\n",
    "        rng.integers(0, 90, size=n_drivers), unit=\"D\"\n",
    "    )\n",
    "\n",
    "    drivers = pd.DataFrame({\n",
    "        \"driver_id\": driver_ids,\n",
    "        \"name\": [f\"Driver_{i}\" for i in driver_ids],\n",
    "        \"rating\": rng.uniform(2.5, 5.0, size=n_drivers).round(2),\n",
    "        \"region\": rng.choice([\"NORTH\", \"SOUTH\", \"EAST\", \"WEST\"], size=n_drivers),\n",
    "        \"created_ts\": base_dates,\n",
    "        \"updated_ts\": base_dates + pd.to_timedelta(\n",
    "            rng.integers(5, 60, size=n_drivers), unit=\"D\"\n",
    "        ),\n",
    "    })\n",
    "\n",
    "    # Shift status\n",
    "    shift_rows = []\n",
    "    shift_id = 1\n",
    "    for idx, row in drivers.iterrows():\n",
    "        did = row[\"driver_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        shift_rows.append({\n",
    "            \"shift_id\": shift_id,\n",
    "            \"driver_ref\": did,\n",
    "            \"shift_start_ts\": base + pd.Timedelta(days=int(rng.integers(0, 10))),\n",
    "            \"shift_end_ts\": base + pd.Timedelta(days=int(rng.integers(10, 20))),\n",
    "        })\n",
    "        shift_id += 1\n",
    "    driver_shift_status = pd.DataFrame(shift_rows)\n",
    "\n",
    "    # Location updates\n",
    "    loc_rows = []\n",
    "    loc_id = 1\n",
    "    for idx, row in drivers.iterrows():\n",
    "        did = row[\"driver_id\"]\n",
    "        base = row[\"created_ts\"]\n",
    "        n_ping = rng.integers(1, 5)\n",
    "        for _ in range(n_ping):\n",
    "            loc_rows.append({\n",
    "                \"location_update_id\": loc_id,\n",
    "                \"driver_ref\": did,\n",
    "                \"lat\": rng.uniform(-90, 90),\n",
    "                \"lon\": rng.uniform(-180, 180),\n",
    "                \"ping_ts\": base + pd.Timedelta(days=int(rng.integers(0, 30))),\n",
    "            })\n",
    "            loc_id += 1\n",
    "    driver_location_updates = pd.DataFrame(loc_rows)\n",
    "\n",
    "    return drivers, driver_shift_status, driver_location_updates\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 4. MAIN ORDER + EVENT GENERATION\n",
    "# ================================================================\n",
    "\n",
    "def generate_orders_and_events_medium(\n",
    "    n_traces: int,\n",
    "    customers: pd.DataFrame,\n",
    "    customer_addresses: pd.DataFrame,\n",
    "    customer_payment_methods: pd.DataFrame,\n",
    "    merchants: pd.DataFrame,\n",
    "    merchant_locations: pd.DataFrame,\n",
    "    menu_items: pd.DataFrame,\n",
    "    drivers: pd.DataFrame,\n",
    ") -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame]:\n",
    "\n",
    "    # --- Prepare lookups ---\n",
    "    addr_by_cust = customer_addresses.groupby(\"cust_ref\")[\"address_id\"].apply(list).to_dict()\n",
    "    pay_by_cust = customer_payment_methods.groupby(\"cust_ref\")[\"pay_method_id\"].apply(list).to_dict()\n",
    "    loc_ids = merchant_locations[\"location_id\"].tolist()\n",
    "    driver_ids = drivers[\"driver_id\"].tolist()\n",
    "\n",
    "    customers_idx = customers.set_index(\"customer_id\")\n",
    "    payment_idx = customer_payment_methods.set_index(\"pay_method_id\")\n",
    "    menu_by_loc = menu_items.groupby(\"location_ref\")\n",
    "\n",
    "    # --- Collectors for ~50+ tables ---\n",
    "    orders = []\n",
    "    order_items = []\n",
    "    order_discounts = []\n",
    "    order_status_events = []\n",
    "    order_tax_calculation = []\n",
    "    order_promo_validation = []\n",
    "    order_cart_snapshots = []\n",
    "    order_experiments = []\n",
    "\n",
    "    risk_requests = []\n",
    "    risk_scores = []\n",
    "    fraud_checks = []\n",
    "    manual_review_cases = []\n",
    "\n",
    "    kitchen_tickets = []\n",
    "    kitchen_events = []\n",
    "\n",
    "    dispatch_requests = []\n",
    "    driver_assignments = []\n",
    "    delivery_status_events = []\n",
    "    delivery_package_scans = []\n",
    "    handover_confirmations = []\n",
    "\n",
    "    delivery_exceptions = []\n",
    "    reassignment_requests = []\n",
    "    fallback_delivery_methods = []\n",
    "\n",
    "    payment_authorizations = []\n",
    "    payment_captures = []\n",
    "    payment_settlements = []\n",
    "    refund_requests = []\n",
    "    refund_approvals = []\n",
    "    refund_settlements = []\n",
    "\n",
    "    support_tickets = []\n",
    "    support_ticket_messages = []\n",
    "    support_issue_resolutions = []\n",
    "\n",
    "    customer_reviews = []\n",
    "    customer_rewards = []\n",
    "    system_audit_logs = []\n",
    "    user_action_logs = []\n",
    "\n",
    "    route_plans = []\n",
    "    route_segments = []\n",
    "    traffic_incidents = []\n",
    "\n",
    "    # ID counters\n",
    "    order_id = 1\n",
    "    order_status_id = 1\n",
    "    tax_id = 1\n",
    "    promo_id = 1\n",
    "    cart_id = 1\n",
    "    exp_id = 1\n",
    "    order_item_id = 1\n",
    "    discount_id = 1\n",
    "\n",
    "    risk_req_id = 1\n",
    "    risk_score_id = 1\n",
    "    fraud_id = 1\n",
    "    manual_review_id = 1\n",
    "\n",
    "    kitchen_ticket_id = 1\n",
    "    kitchen_event_id = 1\n",
    "\n",
    "    dispatch_req_id = 1\n",
    "    driver_assign_id = 1\n",
    "    delivery_evt_id = 1\n",
    "    pkg_scan_id = 1\n",
    "    handover_id = 1\n",
    "\n",
    "    del_ex_id = 1\n",
    "    reassignment_id = 1\n",
    "    fallback_id = 1\n",
    "\n",
    "    auth_id = 1\n",
    "    capture_id = 1\n",
    "    settle_id = 1\n",
    "    refund_req_id = 1\n",
    "    refund_app_id = 1\n",
    "    refund_settle_id = 1\n",
    "\n",
    "    ticket_id = 1\n",
    "    ticket_msg_id = 1\n",
    "    issue_res_id = 1\n",
    "\n",
    "    review_id = 1\n",
    "    reward_id = 1\n",
    "    audit_id = 1\n",
    "    user_action_id = 1\n",
    "\n",
    "    route_plan_id = 1\n",
    "    segment_id = 1\n",
    "    incident_id = 1\n",
    "\n",
    "    # Event traces\n",
    "    event_trace_rows = []\n",
    "\n",
    "    for _ in range(n_traces):\n",
    "        # --- Choose base entities ---\n",
    "        cust_id = int(rng.choice(customers[\"customer_id\"].to_numpy()))\n",
    "        cust_row = customers_idx.loc[cust_id]\n",
    "\n",
    "        addr_id = int(rng.choice(addr_by_cust[cust_id]))\n",
    "        pay_id = int(rng.choice(pay_by_cust[cust_id]))\n",
    "        pay_row = payment_idx.loc[pay_id]\n",
    "\n",
    "        loc_id = int(rng.choice(loc_ids))\n",
    "        loc_menu = menu_by_loc.get_group(loc_id)\n",
    "        menu_row = loc_menu.sample(1).iloc[0]\n",
    "\n",
    "        driver_id = int(rng.choice(driver_ids))\n",
    "\n",
    "        # Base timestamp for trace\n",
    "        base_ts = pd.Timestamp(\"2024-03-01\") + pd.to_timedelta(\n",
    "            int(rng.integers(0, 60)), unit=\"D\"\n",
    "        )\n",
    "        next_ts = make_ts_advancer(base_ts)\n",
    "\n",
    "        events: List[str] = []\n",
    "        join_path: List[str] = []\n",
    "        key_uuid = str(uuid.uuid4())\n",
    "\n",
    "        # --- CUSTOMER + MERCHANT EVENTS (fixed) ---\n",
    "\n",
    "        # CustomerCreated\n",
    "        events.append(\"CustomerCreated\")\n",
    "        join_path.append(\"customers.customer_id\")\n",
    "        ts_cust_created = next_ts(1)\n",
    "\n",
    "        # AddressAdded\n",
    "        events.append(\"AddressAdded\")\n",
    "        join_path.append(\"customer_addresses.address_id\")\n",
    "        ts_addr_added = next_ts(1)\n",
    "\n",
    "        # DeviceRegistered (logged in user_action_logs)\n",
    "        events.append(\"DeviceRegistered\")\n",
    "        join_path.append(\"user_action_logs.action_id\")\n",
    "        ts_device = next_ts(1)\n",
    "        user_action_logs.append({\n",
    "            \"action_id\": user_action_id,\n",
    "            \"cust_ref\": cust_id,\n",
    "            \"action_type\": \"DEVICE_REGISTERED\",\n",
    "            \"created_ts\": ts_device,\n",
    "        })\n",
    "        user_action_id += 1\n",
    "\n",
    "        # MerchantCreated (audit log)\n",
    "        events.append(\"MerchantCreated\")\n",
    "        join_path.append(\"system_audit_logs.audit_id\")\n",
    "        ts_merch = next_ts(1)\n",
    "        system_audit_logs.append({\n",
    "            \"audit_id\": audit_id,\n",
    "            \"entity_type\": \"MERCHANT\",\n",
    "            \"entity_id\": loc_id,\n",
    "            \"action\": \"MERCHANT_USED_FOR_ORDER\",\n",
    "            \"created_ts\": ts_merch,\n",
    "        })\n",
    "        audit_id += 1\n",
    "\n",
    "        # LocationActivated (audit log)\n",
    "        events.append(\"LocationActivated\")\n",
    "        join_path.append(\"system_audit_logs.audit_id\")\n",
    "        ts_loc = next_ts(1)\n",
    "        system_audit_logs.append({\n",
    "            \"audit_id\": audit_id,\n",
    "            \"entity_type\": \"LOCATION\",\n",
    "            \"entity_id\": loc_id,\n",
    "            \"action\": \"LOCATION_ACTIVE\",\n",
    "            \"created_ts\": ts_loc,\n",
    "        })\n",
    "        audit_id += 1\n",
    "\n",
    "        # --- ORDER CREATION SEGMENT ---\n",
    "\n",
    "        # OrderCreated\n",
    "        events.append(\"OrderCreated\")\n",
    "        join_path.append(\"orders.order_id\")\n",
    "        ts_order_created = next_ts(1)\n",
    "\n",
    "        quantity = int(rng.integers(1, 6))\n",
    "        item_price = float(menu_row[\"base_price\"])\n",
    "        subtotal = quantity * item_price\n",
    "\n",
    "        orders.append({\n",
    "            \"order_id\": order_id,\n",
    "            \"cust_ref\": cust_id,\n",
    "            \"location_ref\": loc_id,\n",
    "            \"pay_method_ref\": pay_id,\n",
    "            \"primary_address_ref\": addr_id,\n",
    "            \"order_amount\": subtotal,\n",
    "            \"created_ts\": ts_order_created,\n",
    "            \"updated_ts\": ts_order_created,\n",
    "        })\n",
    "\n",
    "        # CartSnapshotSaved\n",
    "        events.append(\"CartSnapshotSaved\")\n",
    "        join_path.append(\"order_cart_snapshots.snapshot_id\")\n",
    "        ts_cart = next_ts(1)\n",
    "        order_cart_snapshots.append({\n",
    "            \"snapshot_id\": cart_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"created_ts\": ts_cart,\n",
    "        })\n",
    "        cart_id += 1\n",
    "\n",
    "        # ExperimentBucketAssigned\n",
    "        events.append(\"ExperimentBucketAssigned\")\n",
    "        join_path.append(\"order_experiments.experiment_id\")\n",
    "        ts_exp = next_ts(1)\n",
    "        bucket = rng.choice([\"A\", \"B\", \"C\"])\n",
    "        order_experiments.append({\n",
    "            \"experiment_id\": exp_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"bucket\": bucket,\n",
    "            \"assigned_ts\": ts_exp,\n",
    "        })\n",
    "        exp_id += 1\n",
    "\n",
    "        # PromoValidated\n",
    "        events.append(\"PromoValidated\")\n",
    "        join_path.append(\"order_promo_validation.promo_id\")\n",
    "        ts_promo = next_ts(1)\n",
    "        discount = float(rng.uniform(0, 10))\n",
    "        order_promo_validation.append({\n",
    "            \"promo_id\": promo_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"valid\": True,\n",
    "            \"discount_amount\": discount,\n",
    "            \"validated_ts\": ts_promo,\n",
    "        })\n",
    "        promo_id += 1\n",
    "\n",
    "        if discount > 0:\n",
    "            order_discounts.append({\n",
    "                \"discount_id\": discount_id,\n",
    "                \"order_ref\": order_id,\n",
    "                \"discount_amount\": discount,\n",
    "                \"created_ts\": ts_promo,\n",
    "            })\n",
    "            discount_id += 1\n",
    "\n",
    "        # TaxCalculated\n",
    "        events.append(\"TaxCalculated\")\n",
    "        join_path.append(\"order_tax_calculation.tax_id\")\n",
    "        ts_tax = next_ts(1)\n",
    "        tax_amount = round(subtotal * 0.08, 2)\n",
    "        order_tax_calculation.append({\n",
    "            \"tax_id\": tax_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"tax_amount\": tax_amount,\n",
    "            \"calculated_ts\": ts_tax,\n",
    "        })\n",
    "        tax_id += 1\n",
    "\n",
    "        # OrderConfirmed\n",
    "        events.append(\"OrderConfirmed\")\n",
    "        join_path.append(\"order_status_events.status_event_id\")\n",
    "        ts_conf = next_ts(1)\n",
    "        order_status_events.append({\n",
    "            \"status_event_id\": order_status_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"status\": \"CONFIRMED\",\n",
    "            \"event_ts\": ts_conf,\n",
    "        })\n",
    "        order_status_id += 1\n",
    "\n",
    "        # Order items\n",
    "        order_items.append({\n",
    "            \"order_item_id\": order_item_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"menu_ref\": int(menu_row[\"item_id\"]),\n",
    "            \"quantity\": quantity,\n",
    "            \"unit_price\": item_price,\n",
    "            \"created_ts\": ts_order_created,\n",
    "        })\n",
    "        order_item_id += 1\n",
    "\n",
    "        # --- RISK SEGMENT (Branch 1) ---\n",
    "\n",
    "        # RiskCheckStarted\n",
    "        events.append(\"RiskCheckStarted\")\n",
    "        join_path.append(\"risk_requests.risk_request_id\")\n",
    "        ts_risk_start = next_ts(1)\n",
    "        risk_requests.append({\n",
    "            \"risk_request_id\": risk_req_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"created_ts\": ts_risk_start,\n",
    "        })\n",
    "\n",
    "        # RiskScoreCalculated\n",
    "        events.append(\"RiskScoreCalculated\")\n",
    "        join_path.append(\"risk_scores.risk_score_id\")\n",
    "        ts_risk_score = next_ts(1)\n",
    "\n",
    "        # Deterministic risk_level based on customer + wallet + amount\n",
    "        wallet_balance = float(pay_row[\"wallet_balance\"])\n",
    "        outstanding = float(cust_row[\"outstanding_amount\"])\n",
    "        credit = int(cust_row[\"credit_rating\"])\n",
    "\n",
    "        raw_score = (\n",
    "            (subtotal / 100.0) +\n",
    "            (outstanding / 200.0) -\n",
    "            (wallet_balance / 300.0) +\n",
    "            (6 - credit) * 0.5\n",
    "        )\n",
    "        if raw_score < 1.0:\n",
    "            risk_level = \"LOW\"\n",
    "        elif raw_score < 2.0:\n",
    "            risk_level = \"MEDIUM\"\n",
    "        else:\n",
    "            risk_level = \"HIGH\"\n",
    "\n",
    "        risk_scores.append({\n",
    "            \"risk_score_id\": risk_score_id,\n",
    "            \"risk_request_ref\": risk_req_id,\n",
    "            \"risk_level\": risk_level,\n",
    "            \"score_value\": raw_score,\n",
    "            \"calculated_ts\": ts_risk_score,\n",
    "        })\n",
    "\n",
    "        # FraudCheckPerformed\n",
    "        events.append(\"FraudCheckPerformed\")\n",
    "        join_path.append(\"fraud_checks.fraud_check_id\")\n",
    "        ts_fraud = next_ts(1)\n",
    "        fraud_checks.append({\n",
    "            \"fraud_check_id\": fraud_id,\n",
    "            \"risk_request_ref\": risk_req_id,\n",
    "            \"rule_engine_version\": \"v1.0\",\n",
    "            \"check_ts\": ts_fraud,\n",
    "        })\n",
    "        fraud_id += 1\n",
    "\n",
    "        # RiskCheckCompleted\n",
    "        events.append(\"RiskCheckCompleted\")\n",
    "        join_path.append(\"risk_scores.risk_score_id\")\n",
    "        ts_risk_done = next_ts(1)\n",
    "\n",
    "        # Branch 1: Auto vs Manual\n",
    "        if risk_level in [\"LOW\", \"MEDIUM\"]:\n",
    "            # AutoApprovalGranted\n",
    "            events.append(\"AutoApprovalGranted\")\n",
    "            join_path.append(\"risk_scores.risk_score_id\")\n",
    "            ts_auto = next_ts(1)\n",
    "        else:\n",
    "            # ManualReviewRequired\n",
    "            events.append(\"ManualReviewRequired\")\n",
    "            join_path.append(\"manual_review_cases.review_id\")\n",
    "            ts_mr_req = next_ts(1)\n",
    "            manual_review_cases.append({\n",
    "                \"review_id\": manual_review_id,\n",
    "                \"risk_score_ref\": risk_score_id,\n",
    "                \"created_ts\": ts_mr_req,\n",
    "                \"status\": \"REQUIRED\",\n",
    "            })\n",
    "\n",
    "            # ManualReviewCompleted (always approve in this clean version)\n",
    "            events.append(\"ManualReviewCompleted\")\n",
    "            join_path.append(\"manual_review_cases.review_id\")\n",
    "            ts_mr_done = next_ts(3)\n",
    "            manual_review_cases[-1][\"completed_ts\"] = ts_mr_done\n",
    "            manual_review_cases[-1][\"status\"] = \"APPROVED\"\n",
    "            manual_review_id += 1\n",
    "\n",
    "        risk_req_id += 1\n",
    "        risk_score_id += 1\n",
    "\n",
    "        # --- KITCHEN / DISPATCH / DELIVERY ---\n",
    "\n",
    "        # KitchenTicketCreated\n",
    "        events.append(\"KitchenTicketCreated\")\n",
    "        join_path.append(\"kitchen_tickets.ticket_id\")\n",
    "        ts_kt = next_ts(1)\n",
    "        kitchen_tickets.append({\n",
    "            \"ticket_id\": kitchen_ticket_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"created_ts\": ts_kt,\n",
    "        })\n",
    "\n",
    "        # KitchenCookingStarted\n",
    "        events.append(\"KitchenCookingStarted\")\n",
    "        join_path.append(\"kitchen_events.kitchen_event_id\")\n",
    "        ts_kstart = next_ts(5)\n",
    "        kitchen_events.append({\n",
    "            \"kitchen_event_id\": kitchen_event_id,\n",
    "            \"ticket_ref\": kitchen_ticket_id,\n",
    "            \"event_type\": \"COOKING_STARTED\",\n",
    "            \"event_ts\": ts_kstart,\n",
    "        })\n",
    "        kitchen_event_id += 1\n",
    "\n",
    "        # KitchenCookingFinished\n",
    "        events.append(\"KitchenCookingFinished\")\n",
    "        join_path.append(\"kitchen_events.kitchen_event_id\")\n",
    "        ts_kdone = next_ts(15)\n",
    "        kitchen_events.append({\n",
    "            \"kitchen_event_id\": kitchen_event_id,\n",
    "            \"ticket_ref\": kitchen_ticket_id,\n",
    "            \"event_type\": \"COOKING_FINISHED\",\n",
    "            \"event_ts\": ts_kdone,\n",
    "        })\n",
    "        kitchen_event_id += 1\n",
    "        kitchen_ticket_id += 1\n",
    "\n",
    "        # DispatchRequested\n",
    "        events.append(\"DispatchRequested\")\n",
    "        join_path.append(\"dispatch_requests.dispatch_request_id\")\n",
    "        ts_disp = next_ts(1)\n",
    "        dispatch_requests.append({\n",
    "            \"dispatch_request_id\": dispatch_req_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"created_ts\": ts_disp,\n",
    "        })\n",
    "\n",
    "        # DriverShiftOnline (simplified log via assignment)\n",
    "        events.append(\"DriverShiftOnline\")\n",
    "        join_path.append(\"driver_assignments.assignment_id\")\n",
    "        ts_shift = next_ts(1)\n",
    "\n",
    "        # DriverAssigned\n",
    "        events.append(\"DriverAssigned\")\n",
    "        join_path.append(\"driver_assignments.assignment_id\")\n",
    "        ts_assigned = next_ts(1)\n",
    "        driver_assignments.append({\n",
    "            \"assignment_id\": driver_assign_id,\n",
    "            \"dispatch_ref\": dispatch_req_id,\n",
    "            \"driver_ref\": driver_id,\n",
    "            \"assigned_ts\": ts_assigned,\n",
    "            \"updated_ts\": ts_assigned,\n",
    "        })\n",
    "        dispatch_req_id += 1\n",
    "\n",
    "        # RoutePlanned (create route_plan)\n",
    "        events.append(\"RoutePlanned\")\n",
    "        join_path.append(\"route_plans.route_plan_id\")\n",
    "        ts_route = next_ts(2)\n",
    "        route_plans.append({\n",
    "            \"route_plan_id\": route_plan_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"driver_ref\": driver_id,\n",
    "            \"created_ts\": ts_route,\n",
    "            \"updated_ts\": ts_route,\n",
    "        })\n",
    "\n",
    "        # Delivery status event for route planned\n",
    "        delivery_status_events.append({\n",
    "            \"delivery_event_id\": delivery_evt_id,\n",
    "            \"assignment_ref\": driver_assign_id,\n",
    "            \"status\": \"ROUTE_PLANNED\",\n",
    "            \"event_ts\": ts_route,\n",
    "        })\n",
    "        delivery_evt_id += 1\n",
    "\n",
    "        # SegmentStarted (segment #1)\n",
    "        events.append(\"SegmentStarted\")\n",
    "        join_path.append(\"delivery_status_events.delivery_event_id\")\n",
    "        ts_seg_start = next_ts(2)\n",
    "        delivery_status_events.append({\n",
    "            \"delivery_event_id\": delivery_evt_id,\n",
    "            \"assignment_ref\": driver_assign_id,\n",
    "            \"status\": \"SEGMENT_STARTED\",\n",
    "            \"event_ts\": ts_seg_start,\n",
    "        })\n",
    "        delivery_evt_id += 1\n",
    "\n",
    "        # SegmentCompleted (segment #1)\n",
    "        events.append(\"SegmentCompleted\")\n",
    "        join_path.append(\"delivery_status_events.delivery_event_id\")\n",
    "        ts_seg_done = next_ts(5)\n",
    "        delivery_status_events.append({\n",
    "            \"delivery_event_id\": delivery_evt_id,\n",
    "            \"assignment_ref\": driver_assign_id,\n",
    "            \"status\": \"SEGMENT_COMPLETED\",\n",
    "            \"event_ts\": ts_seg_done,\n",
    "        })\n",
    "        delivery_evt_id += 1\n",
    "\n",
    "        # Register route segment in route_segments table\n",
    "        route_segments.append({\n",
    "            \"segment_id\": segment_id,\n",
    "            \"route_plan_ref\": route_plan_id,\n",
    "            \"sequence_no\": 1,\n",
    "            \"start_ts\": ts_seg_start,\n",
    "            \"end_ts\": ts_seg_done,\n",
    "        })\n",
    "        segment_id += 1\n",
    "\n",
    "        # Delivered\n",
    "        events.append(\"Delivered\")\n",
    "        join_path.append(\"handover_confirmations.handover_id\")\n",
    "        ts_delivered = next_ts(5)\n",
    "        handover_confirmations.append({\n",
    "            \"handover_id\": handover_id,\n",
    "            \"assignment_ref\": driver_assign_id,\n",
    "            \"confirmed_ts\": ts_delivered,\n",
    "        })\n",
    "        handover_id += 1\n",
    "\n",
    "        # Package scan log\n",
    "        delivery_package_scans.append({\n",
    "            \"package_scan_id\": pkg_scan_id,\n",
    "            \"assignment_ref\": driver_assign_id,\n",
    "            \"scan_ts\": ts_delivered,\n",
    "        })\n",
    "        pkg_scan_id += 1\n",
    "\n",
    "        driver_assign_id += 1\n",
    "        route_plan_id += 1\n",
    "\n",
    "        # --- DELIVERY EXCEPTION BRANCH (Branch 2) ---\n",
    "\n",
    "        driver_rating = float(drivers.loc[drivers[\"driver_id\"] == driver_id, \"rating\"].iloc[0])\n",
    "\n",
    "        # Simple route risk score\n",
    "        route_risk_score = 0.0\n",
    "        if risk_level == \"HIGH\":\n",
    "            route_risk_score += 1.0\n",
    "        if driver_rating < 3.5:\n",
    "            route_risk_score += 0.8\n",
    "        if cust_row[\"region\"] == \"NORTH\":\n",
    "            route_risk_score += 0.4\n",
    "\n",
    "        # Exception occurs if route_risk_score >= 1.2\n",
    "        if route_risk_score >= 1.2:\n",
    "            events.append(\"DeliveryExceptionOccurred\")\n",
    "            join_path.append(\"delivery_exceptions.exception_id\")\n",
    "            ts_exc = next_ts(1)\n",
    "\n",
    "            # Determine exception_type deterministically\n",
    "            if driver_rating < 3.0:\n",
    "                exception_type = \"REASSIGN_DRIVER\"\n",
    "            elif cust_row[\"region\"] == \"NORTH\":\n",
    "                exception_type = \"REROUTE\"\n",
    "            elif route_risk_score > 1.8:\n",
    "                exception_type = \"CANCEL\"\n",
    "            else:\n",
    "                exception_type = \"DELAY\"\n",
    "\n",
    "            delivery_exceptions.append({\n",
    "                \"exception_id\": del_ex_id,\n",
    "                \"assignment_ref\": driver_assign_id - 1,\n",
    "                \"exception_type\": exception_type,\n",
    "                \"exception_ts\": ts_exc,\n",
    "            })\n",
    "\n",
    "            # For REROUTE/DELAY, we can log traffic_incidents\n",
    "            if exception_type in [\"REROUTE\", \"DELAY\"]:\n",
    "                traffic_incidents.append({\n",
    "                    \"incident_id\": incident_id,\n",
    "                    \"route_plan_ref\": route_plan_id - 1,\n",
    "                    \"incident_ts\": ts_exc,\n",
    "                    \"severity\": rng.choice([\"LOW\", \"MEDIUM\", \"HIGH\"]),\n",
    "                })\n",
    "                incident_id += 1\n",
    "\n",
    "            # Branch 2: 4 routes\n",
    "            if exception_type == \"REROUTE\":\n",
    "                events.append(\"ReroutePlanned\")\n",
    "                join_path.append(\"fallback_delivery_methods.fallback_id\")\n",
    "                ts_reroute = next_ts(2)\n",
    "                fallback_delivery_methods.append({\n",
    "                    \"fallback_id\": fallback_id,\n",
    "                    \"assignment_ref\": driver_assign_id - 1,\n",
    "                    \"method\": \"ALT_ROUTE\",\n",
    "                    \"created_ts\": ts_reroute,\n",
    "                })\n",
    "                fallback_id += 1\n",
    "\n",
    "                # New segment #2\n",
    "                events.append(\"SegmentStarted\")\n",
    "                join_path.append(\"delivery_status_events.delivery_event_id\")\n",
    "                ts_seg2_start = next_ts(2)\n",
    "                delivery_status_events.append({\n",
    "                    \"delivery_event_id\": delivery_evt_id,\n",
    "                    \"assignment_ref\": driver_assign_id - 1,\n",
    "                    \"status\": \"SEGMENT2_STARTED\",\n",
    "                    \"event_ts\": ts_seg2_start,\n",
    "                })\n",
    "                delivery_evt_id += 1\n",
    "\n",
    "                events.append(\"SegmentCompleted\")\n",
    "                join_path.append(\"delivery_status_events.delivery_event_id\")\n",
    "                ts_seg2_done = next_ts(5)\n",
    "                delivery_status_events.append({\n",
    "                    \"delivery_event_id\": delivery_evt_id,\n",
    "                    \"assignment_ref\": driver_assign_id - 1,\n",
    "                    \"status\": \"SEGMENT2_COMPLETED\",\n",
    "                    \"event_ts\": ts_seg2_done,\n",
    "                })\n",
    "                delivery_evt_id += 1\n",
    "\n",
    "                # Register route segment #2\n",
    "                route_segments.append({\n",
    "                    \"segment_id\": segment_id,\n",
    "                    \"route_plan_ref\": route_plan_id - 1,\n",
    "                    \"sequence_no\": 2,\n",
    "                    \"start_ts\": ts_seg2_start,\n",
    "                    \"end_ts\": ts_seg2_done,\n",
    "                })\n",
    "                segment_id += 1\n",
    "\n",
    "            elif exception_type == \"DELAY\":\n",
    "                events.append(\"DelayLogged\")\n",
    "                join_path.append(\"delivery_exceptions.exception_id\")\n",
    "                ts_delay = next_ts(5)\n",
    "\n",
    "            elif exception_type == \"REASSIGN_DRIVER\":\n",
    "                events.append(\"DriverReassigned\")\n",
    "                join_path.append(\"reassignment_requests.reassignment_id\")\n",
    "                ts_reassign = next_ts(3)\n",
    "                new_driver = int(rng.choice(driver_ids))\n",
    "                reassignment_requests.append({\n",
    "                    \"reassignment_id\": reassignment_id,\n",
    "                    \"old_assignment_ref\": driver_assign_id - 1,\n",
    "                    \"new_driver_ref\": new_driver,\n",
    "                    \"created_ts\": ts_reassign,\n",
    "                })\n",
    "                reassignment_id += 1\n",
    "\n",
    "                # Continue with new route (secondary route plan)\n",
    "                events.append(\"RoutePlanned\")\n",
    "                join_path.append(\"route_plans.route_plan_id\")\n",
    "                ts_route2 = next_ts(3)\n",
    "                route_plans.append({\n",
    "                    \"route_plan_id\": route_plan_id,\n",
    "                    \"order_ref\": order_id,\n",
    "                    \"driver_ref\": new_driver,\n",
    "                    \"created_ts\": ts_route2,\n",
    "                    \"updated_ts\": ts_route2,\n",
    "                })\n",
    "\n",
    "                delivery_status_events.append({\n",
    "                    \"delivery_event_id\": delivery_evt_id,\n",
    "                    \"assignment_ref\": driver_assign_id - 1,\n",
    "                    \"status\": \"ROUTE_REPLANNED\",\n",
    "                    \"event_ts\": ts_route2,\n",
    "                })\n",
    "                delivery_evt_id += 1\n",
    "\n",
    "                route_plan_id += 1\n",
    "\n",
    "            elif exception_type == \"CANCEL\":\n",
    "                events.append(\"OrderCancelled\")\n",
    "                join_path.append(\"order_status_events.status_event_id\")\n",
    "                ts_cancel = next_ts(1)\n",
    "                order_status_events.append({\n",
    "                    \"status_event_id\": order_status_id,\n",
    "                    \"order_ref\": order_id,\n",
    "                    \"status\": \"CANCELLED_AFTER_DELIVERY_ISSUE\",\n",
    "                    \"event_ts\": ts_cancel,\n",
    "                })\n",
    "                order_status_id += 1\n",
    "\n",
    "                # END TRACE HERE\n",
    "                event_trace_rows.append({\n",
    "                    \"Key_Selector\": \"Order_ID\",\n",
    "                    \"Key_ID\": key_uuid,\n",
    "                    \"Event_Trace\": str(events),\n",
    "                    \"Join_Path\": str(join_path),\n",
    "                })\n",
    "                del_ex_id += 1\n",
    "                order_id += 1\n",
    "                continue\n",
    "\n",
    "            del_ex_id += 1\n",
    "\n",
    "        # --- PAYMENT / REFUND / SUPPORT / REVIEW ---\n",
    "\n",
    "        # PaymentAuthorized\n",
    "        events.append(\"PaymentAuthorized\")\n",
    "        join_path.append(\"payment_authorizations.auth_id\")\n",
    "        ts_auth = next_ts(30)\n",
    "        total_amount = subtotal + tax_amount - discount\n",
    "        payment_authorizations.append({\n",
    "            \"auth_id\": auth_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"amount\": total_amount,\n",
    "            \"created_ts\": ts_auth,\n",
    "        })\n",
    "\n",
    "        # PaymentCaptured\n",
    "        events.append(\"PaymentCaptured\")\n",
    "        join_path.append(\"payment_captures.capture_id\")\n",
    "        ts_cap = next_ts(60)\n",
    "        payment_captures.append({\n",
    "            \"capture_id\": capture_id,\n",
    "            \"auth_ref\": auth_id,\n",
    "            \"captured_amount\": total_amount,\n",
    "            \"captured_ts\": ts_cap,\n",
    "        })\n",
    "\n",
    "        # PaymentSettled\n",
    "        events.append(\"PaymentSettled\")\n",
    "        join_path.append(\"payment_settlements.settlement_id\")\n",
    "        ts_settle = next_ts(24 * 60)  # +1 day\n",
    "        payment_settlements.append({\n",
    "            \"settlement_id\": settle_id,\n",
    "            \"capture_ref\": capture_id,\n",
    "            \"settled_amount\": total_amount,\n",
    "            \"settled_ts\": ts_settle,\n",
    "        })\n",
    "\n",
    "        # Occasionally refund\n",
    "        if total_amount > 40 and rng.random() < 0.2:\n",
    "            events.append(\"RefundRequested\")\n",
    "            join_path.append(\"refund_requests.refund_request_id\")\n",
    "            ts_rreq = next_ts(10)\n",
    "            refund_requests.append({\n",
    "                \"refund_request_id\": refund_req_id,\n",
    "                \"order_ref\": order_id,\n",
    "                \"requested_amount\": total_amount * 0.5,\n",
    "                \"requested_ts\": ts_rreq,\n",
    "            })\n",
    "\n",
    "            events.append(\"RefundApproved\")\n",
    "            join_path.append(\"refund_approvals.refund_approval_id\")\n",
    "            ts_rappr = next_ts(30)\n",
    "            refund_approvals.append({\n",
    "                \"refund_approval_id\": refund_app_id,\n",
    "                \"refund_request_ref\": refund_req_id,\n",
    "                \"approved_ts\": ts_rappr,\n",
    "            })\n",
    "\n",
    "            events.append(\"RefundSettled\")\n",
    "            join_path.append(\"refund_settlements.refund_settlement_id\")\n",
    "            ts_rsettle = next_ts(24 * 60)\n",
    "            refund_settlements.append({\n",
    "                \"refund_settlement_id\": refund_settle_id,\n",
    "                \"refund_approval_ref\": refund_app_id,\n",
    "                \"settled_ts\": ts_rsettle,\n",
    "            })\n",
    "\n",
    "            refund_req_id += 1\n",
    "            refund_app_id += 1\n",
    "            refund_settle_id += 1\n",
    "\n",
    "        auth_id += 1\n",
    "        capture_id += 1\n",
    "        settle_id += 1\n",
    "\n",
    "        # Support path (sometimes)\n",
    "        if rng.random() < 0.3:\n",
    "            events.append(\"SupportTicketOpened\")\n",
    "            join_path.append(\"support_tickets.ticket_id\")\n",
    "            ts_topn = next_ts(60)\n",
    "            support_tickets.append({\n",
    "                \"ticket_id\": ticket_id,\n",
    "                \"order_ref\": order_id,\n",
    "                \"opened_ts\": ts_topn,\n",
    "            })\n",
    "\n",
    "            events.append(\"SupportMessageAdded\")\n",
    "            join_path.append(\"support_ticket_messages.message_id\")\n",
    "            ts_msg = next_ts(10)\n",
    "            support_ticket_messages.append({\n",
    "                \"message_id\": ticket_msg_id,\n",
    "                \"ticket_ref\": ticket_id,\n",
    "                \"created_ts\": ts_msg,\n",
    "            })\n",
    "            ticket_msg_id += 1\n",
    "\n",
    "            events.append(\"SupportIssueResolved\")\n",
    "            join_path.append(\"support_issue_resolutions.resolution_id\")\n",
    "            ts_res = next_ts(50)\n",
    "            support_issue_resolutions.append({\n",
    "                \"resolution_id\": issue_res_id,\n",
    "                \"ticket_ref\": ticket_id,\n",
    "                \"resolved_ts\": ts_res,\n",
    "            })\n",
    "            issue_res_id += 1\n",
    "            ticket_id += 1\n",
    "\n",
    "        # ReviewSubmitted\n",
    "        events.append(\"ReviewSubmitted\")\n",
    "        join_path.append(\"customer_reviews.review_id\")\n",
    "        ts_review = next_ts(60)\n",
    "        rating = int(rng.integers(1, 6))\n",
    "        customer_reviews.append({\n",
    "            \"review_id\": review_id,\n",
    "            \"order_ref\": order_id,\n",
    "            \"rating\": rating,\n",
    "            \"created_ts\": ts_review,\n",
    "        })\n",
    "        review_id += 1\n",
    "\n",
    "        # RewardCredited\n",
    "        events.append(\"RewardCredited\")\n",
    "        join_path.append(\"customer_rewards.reward_id\")\n",
    "        ts_reward = next_ts(5)\n",
    "        customer_rewards.append({\n",
    "            \"reward_id\": reward_id,\n",
    "            \"cust_ref\": cust_id,\n",
    "            \"points\": max(1, rating) * 10,\n",
    "            \"credited_ts\": ts_reward,\n",
    "        })\n",
    "        reward_id += 1\n",
    "\n",
    "        # AuditLogWritten\n",
    "        events.append(\"AuditLogWritten\")\n",
    "        join_path.append(\"system_audit_logs.audit_id\")\n",
    "        ts_audit = next_ts(1)\n",
    "        system_audit_logs.append({\n",
    "            \"audit_id\": audit_id,\n",
    "            \"entity_type\": \"ORDER\",\n",
    "            \"entity_id\": order_id,\n",
    "            \"action\": \"ORDER_COMPLETED\",\n",
    "            \"created_ts\": ts_audit,\n",
    "        })\n",
    "        audit_id += 1\n",
    "\n",
    "        # --- Final trace row ---\n",
    "        event_trace_rows.append({\n",
    "            \"Key_Selector\": \"Order_ID\",\n",
    "            \"Key_ID\": key_uuid,\n",
    "            \"Event_Trace\": str(events),\n",
    "            \"Join_Path\": str(join_path),\n",
    "        })\n",
    "\n",
    "        order_id += 1\n",
    "\n",
    "    # Build all tables dict (57 total)\n",
    "    tables: Dict[str, pd.DataFrame] = {\n",
    "        \"orders\": pd.DataFrame(orders),\n",
    "        \"order_items\": pd.DataFrame(order_items),\n",
    "        \"order_discounts\": pd.DataFrame(order_discounts),\n",
    "        \"order_status_events\": pd.DataFrame(order_status_events),\n",
    "        \"order_tax_calculation\": pd.DataFrame(order_tax_calculation),\n",
    "        \"order_promo_validation\": pd.DataFrame(order_promo_validation),\n",
    "        \"order_cart_snapshots\": pd.DataFrame(order_cart_snapshots),\n",
    "        \"order_experiments\": pd.DataFrame(order_experiments),\n",
    "        \"risk_requests\": pd.DataFrame(risk_requests),\n",
    "        \"risk_scores\": pd.DataFrame(risk_scores),\n",
    "        \"fraud_checks\": pd.DataFrame(fraud_checks),\n",
    "        \"manual_review_cases\": pd.DataFrame(manual_review_cases),\n",
    "        \"kitchen_tickets\": pd.DataFrame(kitchen_tickets),\n",
    "        \"kitchen_events\": pd.DataFrame(kitchen_events),\n",
    "        \"dispatch_requests\": pd.DataFrame(dispatch_requests),\n",
    "        \"driver_assignments\": pd.DataFrame(driver_assignments),\n",
    "        \"delivery_status_events\": pd.DataFrame(delivery_status_events),\n",
    "        \"delivery_package_scans\": pd.DataFrame(delivery_package_scans),\n",
    "        \"handover_confirmations\": pd.DataFrame(handover_confirmations),\n",
    "        \"delivery_exceptions\": pd.DataFrame(delivery_exceptions),\n",
    "        \"reassignment_requests\": pd.DataFrame(reassignment_requests),\n",
    "        \"fallback_delivery_methods\": pd.DataFrame(fallback_delivery_methods),\n",
    "        \"payment_authorizations\": pd.DataFrame(payment_authorizations),\n",
    "        \"payment_captures\": pd.DataFrame(payment_captures),\n",
    "        \"payment_settlements\": pd.DataFrame(payment_settlements),\n",
    "        \"refund_requests\": pd.DataFrame(refund_requests),\n",
    "        \"refund_approvals\": pd.DataFrame(refund_approvals),\n",
    "        \"refund_settlements\": pd.DataFrame(refund_settlements),\n",
    "        \"support_tickets\": pd.DataFrame(support_tickets),\n",
    "        \"support_ticket_messages\": pd.DataFrame(support_ticket_messages),\n",
    "        \"support_issue_resolutions\": pd.DataFrame(support_issue_resolutions),\n",
    "        \"customer_reviews\": pd.DataFrame(customer_reviews),\n",
    "        \"customer_rewards\": pd.DataFrame(customer_rewards),\n",
    "        \"system_audit_logs\": pd.DataFrame(system_audit_logs),\n",
    "        \"user_action_logs\": pd.DataFrame(user_action_logs),\n",
    "        \"route_plans\": pd.DataFrame(route_plans),\n",
    "        \"route_segments\": pd.DataFrame(route_segments),\n",
    "        \"traffic_incidents\": pd.DataFrame(traffic_incidents),\n",
    "    }\n",
    "\n",
    "    event_traces_df = pd.DataFrame(event_trace_rows)\n",
    "\n",
    "    return tables, event_traces_df\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5. FULL MEDIUM DATASET GENERATOR\n",
    "# ================================================================\n",
    "\n",
    "def generate_food_delivery_medium(\n",
    "    n_traces: int = MEDIUM_N_TRACES,\n",
    "    data_dir: str = DATA_DIR_MEDIUM,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate the medium-scale dataset:\n",
    "      - ~57 tables in `data_dir`\n",
    "      - medium_event_traces.csv\n",
    "      - medium_transition_graph.csv\n",
    "    \"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Customers and related tables\n",
    "    (\n",
    "        customers,\n",
    "        customer_addresses,\n",
    "        customer_devices,\n",
    "        customer_payment_methods,\n",
    "        customer_verification,\n",
    "        customer_loyalty_status,\n",
    "    ) = generate_customers_medium()\n",
    "\n",
    "    # Merchants, locations, menu, inventory, updates\n",
    "    (\n",
    "        merchants,\n",
    "        merchant_locations,\n",
    "        merchant_business_docs,\n",
    "        merchant_bank_accounts,\n",
    "        merchant_operational_status,\n",
    "        menu_items,\n",
    "        menu_item_options,\n",
    "        inventory_reservations,\n",
    "        inventory_adjustments,\n",
    "        merchant_menu_updates,\n",
    "    ) = generate_merchants_and_menu_medium()\n",
    "\n",
    "    # Drivers\n",
    "    (\n",
    "        drivers,\n",
    "        driver_shift_status,\n",
    "        driver_location_updates,\n",
    "    ) = generate_drivers_and_logistics_medium()\n",
    "\n",
    "    # Orders + main fact tables + event traces\n",
    "    fact_tables, event_traces = generate_orders_and_events_medium(\n",
    "        n_traces=n_traces,\n",
    "        customers=customers,\n",
    "        customer_addresses=customer_addresses,\n",
    "        customer_payment_methods=customer_payment_methods,\n",
    "        merchants=merchants,\n",
    "        merchant_locations=merchant_locations,\n",
    "        menu_items=menu_items,\n",
    "        drivers=drivers,\n",
    "    )\n",
    "\n",
    "    # Transition graph\n",
    "    transition_graph = build_medium_transition_graph()\n",
    "\n",
    "    # Save event traces & transition graph at root\n",
    "    event_traces.to_csv(\"medium_event_traces.csv\", index=False)\n",
    "    transition_graph.to_csv(\"medium_transition_graph.csv\", index=False)\n",
    "\n",
    "    # Save dimension tables\n",
    "    customers.to_csv(os.path.join(data_dir, \"customers.csv\"), index=False)\n",
    "    customer_addresses.to_csv(os.path.join(data_dir, \"customer_addresses.csv\"), index=False)\n",
    "    customer_devices.to_csv(os.path.join(data_dir, \"customer_devices.csv\"), index=False)\n",
    "    customer_payment_methods.to_csv(\n",
    "        os.path.join(data_dir, \"customer_payment_methods.csv\"), index=False\n",
    "    )\n",
    "    customer_verification.to_csv(\n",
    "        os.path.join(data_dir, \"customer_verification.csv\"), index=False\n",
    "    )\n",
    "    customer_loyalty_status.to_csv(\n",
    "        os.path.join(data_dir, \"customer_loyalty_status.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    merchants.to_csv(os.path.join(data_dir, \"merchants.csv\"), index=False)\n",
    "    merchant_locations.to_csv(os.path.join(data_dir, \"merchant_locations.csv\"), index=False)\n",
    "    merchant_business_docs.to_csv(\n",
    "        os.path.join(data_dir, \"merchant_business_docs.csv\"), index=False\n",
    "    )\n",
    "    merchant_bank_accounts.to_csv(\n",
    "        os.path.join(data_dir, \"merchant_bank_accounts.csv\"), index=False\n",
    "    )\n",
    "    merchant_operational_status.to_csv(\n",
    "        os.path.join(data_dir, \"merchant_operational_status.csv\"), index=False\n",
    "    )\n",
    "    menu_items.to_csv(os.path.join(data_dir, \"menu_items.csv\"), index=False)\n",
    "    menu_item_options.to_csv(os.path.join(data_dir, \"menu_item_options.csv\"), index=False)\n",
    "    inventory_reservations.to_csv(\n",
    "        os.path.join(data_dir, \"inventory_reservations.csv\"), index=False\n",
    "    )\n",
    "    inventory_adjustments.to_csv(\n",
    "        os.path.join(data_dir, \"inventory_adjustments.csv\"), index=False\n",
    "    )\n",
    "    merchant_menu_updates.to_csv(\n",
    "        os.path.join(data_dir, \"merchant_menu_updates.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    drivers.to_csv(os.path.join(data_dir, \"drivers.csv\"), index=False)\n",
    "    driver_shift_status.to_csv(\n",
    "        os.path.join(data_dir, \"driver_shift_status.csv\"), index=False\n",
    "    )\n",
    "    driver_location_updates.to_csv(\n",
    "        os.path.join(data_dir, \"driver_location_updates.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Save all fact/log tables (including route_plans/segments/incidents)\n",
    "    for name, df in fact_tables.items():\n",
    "        df.to_csv(os.path.join(data_dir, f\"{name}.csv\"), index=False)\n",
    "\n",
    "    print(f\"Generated {len(event_traces)} traces and wrote tables to '{data_dir}'.\")\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# ENTRY POINT\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_food_delivery_medium(50000, 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6bc836b-b151-4f8c-8e9c-3a7dc5c0f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting noise injection for MEDIUM dataset...\n",
      " Applying 30% schema drift (safe attribute swap)...\n",
      " Applying 15% timestamp missingness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_1207168\\3343414417.py:190: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NaT' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[missing_idx, col] = pd.NaT\n",
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_1207168\\3343414417.py:190: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NaT' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[missing_idx, col] = pd.NaT\n",
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_1207168\\3343414417.py:190: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NaT' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[missing_idx, col] = pd.NaT\n",
      "C:\\Users\\joel\\AppData\\Local\\Temp\\ipykernel_1207168\\3343414417.py:190: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NaT' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[missing_idx, col] = pd.NaT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Noise injection complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Noise Injector for Medium Dataset (57 tables)\n",
    "---------------------------------------------\n",
    "\n",
    "Adds:\n",
    "  - 30% Schema Drift (Attribute Swap)\n",
    "  - 15% Timestamp Missingness\n",
    "\n",
    "Ensures:\n",
    "  - PK/FK safety\n",
    "  - No KeyErrors\n",
    "  - No breakage of join paths\n",
    "  - No event timestamp corruption\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "ATTRIBUTE_SWAP_RATIO = 0.30\n",
    "TIMESTAMP_MISSING_RATIO = 0.15\n",
    "rng = np.random.default_rng(42)\n",
    "random.seed(42)\n",
    "\n",
    "# strings that imply timestamps\n",
    "TS_KEYWORDS = [\"ts\", \"time\", \"date\"]\n",
    "\n",
    "# PK / FK detection heuristics\n",
    "PK_HINTS = [\"id\"]\n",
    "FK_HINTS = [\"_ref\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD / SAVE TABLES\n",
    "# ============================================================\n",
    "\n",
    "def load_tables(data_dir: str) -> dict:\n",
    "    tables = {}\n",
    "    for f in os.listdir(data_dir):\n",
    "        if f.endswith(\".csv\"):\n",
    "            name = f.replace(\".csv\", \"\")\n",
    "            df = pd.read_csv(os.path.join(data_dir, f), parse_dates=True)\n",
    "            tables[name] = df\n",
    "    return tables\n",
    "\n",
    "\n",
    "def save_tables(tables: dict, data_dir: str):\n",
    "    for name, df in tables.items():\n",
    "        df.to_csv(os.path.join(data_dir, f\"{name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# IDENTIFY SAFE ATTRIBUTES\n",
    "# ============================================================\n",
    "\n",
    "def is_timestamp_col(col: str) -> bool:\n",
    "    col_low = col.lower()\n",
    "    return any(tag in col_low for tag in TS_KEYWORDS)\n",
    "\n",
    "\n",
    "def identify_timestamp_columns(df: pd.DataFrame) -> list:\n",
    "    ts_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"datetime64[ns]\":\n",
    "            ts_cols.append(col)\n",
    "        elif is_timestamp_col(col):\n",
    "            ts_cols.append(col)\n",
    "    return ts_cols\n",
    "\n",
    "\n",
    "def identify_pk_fk_cols(df: pd.DataFrame) -> list:\n",
    "    pkfk = []\n",
    "    for col in df.columns:\n",
    "        c = col.lower()\n",
    "        if any(h in c for h in PK_HINTS):\n",
    "            pkfk.append(col)\n",
    "        if any(h in c for h in FK_HINTS):\n",
    "            pkfk.append(col)\n",
    "    return list(set(pkfk))\n",
    "\n",
    "\n",
    "def detect_safe_columns(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Safe means:\n",
    "      - not PK/FK\n",
    "      - not timestamp-like\n",
    "      - not datetime dtype\n",
    "      - not boolean\n",
    "      - not extremely high cardinality (IDs)\n",
    "    \"\"\"\n",
    "    safe = []\n",
    "    pkfk = identify_pk_fk_cols(df)\n",
    "    ts_cols = identify_timestamp_columns(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in pkfk or col in ts_cols:\n",
    "            continue\n",
    "        if df[col].dtype == \"datetime64[ns]\":\n",
    "            continue\n",
    "        if df[col].dtype == bool:\n",
    "            continue\n",
    "\n",
    "        # high-cardinality numeric likely ID\n",
    "        if df[col].dtype != object and df[col].nunique() > len(df) * 0.90:\n",
    "            continue\n",
    "\n",
    "        safe.append(col)\n",
    "\n",
    "    return safe\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ATTRIBUTE SWAP\n",
    "# ============================================================\n",
    "\n",
    "def apply_attribute_swap(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    safe_cols = detect_safe_columns(df)\n",
    "\n",
    "    if len(safe_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    n_drift = max(1, int(len(safe_cols) * ATTRIBUTE_SWAP_RATIO))\n",
    "    cols_to_modify = random.sample(safe_cols, n_drift)\n",
    "\n",
    "    # --- 1. Rename columns ---\n",
    "    rename_map = {}\n",
    "    for col in cols_to_modify:\n",
    "        new_col = f\"attr_{random.randint(100,999)}\"\n",
    "        rename_map[col] = new_col\n",
    "\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    renamed_cols = list(rename_map.values())\n",
    "\n",
    "    # --- 2. Value Swap (if >=2)\n",
    "    if len(renamed_cols) >= 2:\n",
    "        c1, c2 = random.sample(renamed_cols, 2)\n",
    "        df[c1], df[c2] = df[c2], df[c1]\n",
    "\n",
    "    # --- 3. Mild type drift ---\n",
    "    for col in renamed_cols:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(str) + \"_drift\"\n",
    "        else:\n",
    "            df[col] = df[col].astype(float) * rng.uniform(0.7, 1.3)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_attribute_swap_to_all_tables(data_dir: str):\n",
    "    tables = load_tables(data_dir)\n",
    "    drifted = {}\n",
    "\n",
    "    for name, df in tables.items():\n",
    "        # Protect event traces and transition graph\n",
    "        if name in [\"medium_event_traces\", \"medium_transition_graph\"]:\n",
    "            drifted[name] = df\n",
    "            continue\n",
    "\n",
    "        drifted[name] = apply_attribute_swap(df)\n",
    "\n",
    "    save_tables(drifted, data_dir)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TIMESTAMP MISSINGNESS\n",
    "# ============================================================\n",
    "\n",
    "def apply_timestamp_missingness(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    ts_cols = identify_timestamp_columns(df)\n",
    "\n",
    "    if len(ts_cols) == 0:\n",
    "        return df\n",
    "\n",
    "    n = len(df)\n",
    "    n_missing = int(n * TIMESTAMP_MISSING_RATIO)\n",
    "    if n_missing == 0:\n",
    "        return df\n",
    "\n",
    "    missing_idx = rng.choice(df.index, size=n_missing, replace=False)\n",
    "\n",
    "    for col in ts_cols:\n",
    "        df.loc[missing_idx, col] = pd.NaT\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_timestamp_missingness_to_all(data_dir: str):\n",
    "    tables = load_tables(data_dir)\n",
    "    noised = {}\n",
    "\n",
    "    for name, df in tables.items():\n",
    "        if name in [\"medium_event_traces\", \"medium_transition_graph\"]:\n",
    "            noised[name] = df\n",
    "            continue\n",
    "\n",
    "        noised[name] = apply_timestamp_missingness(df)\n",
    "\n",
    "    save_tables(noised, data_dir)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================\n",
    "\n",
    "def inject_noise_medium(data_dir: str = \"./data\"):\n",
    "    print(\" Starting noise injection for MEDIUM dataset...\")\n",
    "    print(\" Applying 30% schema drift (safe attribute swap)...\")\n",
    "    apply_attribute_swap_to_all_tables(data_dir)\n",
    "\n",
    "    print(\" Applying 15% timestamp missingness...\")\n",
    "    apply_timestamp_missingness_to_all(data_dir)\n",
    "\n",
    "    print(\" Noise injection complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inject_noise_medium()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
